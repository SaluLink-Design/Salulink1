{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaluLink-Design/Salulink1/blob/main/Welcome_to_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config globL --user.name \"ThulaMoipolai\"\n",
        "!git config global --email.name \"thulamoipolai@gmail.com\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VmT_Ty9rtkr",
        "outputId": "44ea9f65-a634-4253-f2bf-0aa51c050669"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not in a git directory\n",
            "fatal: not in a git directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/SaluLink-Design/Salulink1.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHqtFE9csBAp",
        "outputId": "36e38651-df23-4615-cee4-1a9e91532753"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Salulink1'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 15 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (15/15), 51.37 KiB | 10.27 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qKQwB4tAtbHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9036f034"
      },
      "source": [
        "# Task\n",
        "Develop an AI model named Authi that analyzes specialist notes using ClinicalBERT, predicts ICD-10 codes, pairs them with PMB information from the \"/content/Salulink1/Brain and nervous system and respiratory system.csv\" dataset, and differentiates between similar codes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a1d9dc3"
      },
      "source": [
        "## Load and prepare data\n",
        "\n",
        "### Subtask:\n",
        "Load the \"Brain and nervous system and respiratory system.csv\" dataset and preprocess it for use in the model, including handling any necessary cleaning or transformations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f05bbbe"
      },
      "source": [
        "**Reasoning**:\n",
        "The first step is to load the data and perform initial inspection and cleaning as outlined in the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "5849bb95",
        "outputId": "03bae687-818c-4bda-a4c2-8c39c3f8ebfb"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/Salulink1/Brain and nervous system and respiratory system.csv\")\n",
        "\n",
        "# Display the first 5 rows\n",
        "print(\"First 5 rows of the DataFrame:\")\n",
        "display(df.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values per column:\")\n",
        "display(df.isnull().sum())\n",
        "\n",
        "# Check data types\n",
        "print(\"\\nData types:\")\n",
        "display(df.dtypes)\n",
        "\n",
        "# Basic cleaning (if necessary, based on the inspection)\n",
        "# For example, stripping whitespace from string columns\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object':\n",
        "        df[col] = df[col].str.strip()\n",
        "\n",
        "print(\"\\nDataFrame after basic cleaning:\")\n",
        "display(df.head())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "'utf-8' codec can't decode byte 0x8e in position 627: invalid start byte",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3584574086.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Salulink1/Brain and nervous system and respiratory system.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Display the first 5 rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Fail here loudly instead of in cython after reading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x8e in position 627: invalid start byte"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2c72c76"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed due to a UnicodeDecodeError. This indicates that the file is not encoded in UTF-8. I need to try reading the file with a different encoding, commonly 'latin1' or 'ISO-8859-1' are used for such issues.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "y7q6KOqW5v5N",
        "outputId": "800f28bc-9548-4856-b91f-4668494a0505"
      },
      "source": [
        "# Load the dataset with a different encoding\n",
        "try:\n",
        "    df = pd.read_csv(\"/content/Salulink1/Brain and nervous system and respiratory system.csv\", encoding='latin1')\n",
        "except UnicodeDecodeError:\n",
        "    df = pd.read_csv(\"/content/Salulink1/Brain and nervous system and respiratory system.csv\", encoding='ISO-8859-1')\n",
        "\n",
        "\n",
        "# Display the first 5 rows\n",
        "print(\"First 5 rows of the DataFrame:\")\n",
        "display(df.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values per column:\")\n",
        "display(df.isnull().sum())\n",
        "\n",
        "# Check data types\n",
        "print(\"\\nData types:\")\n",
        "display(df.dtypes)\n",
        "\n",
        "# Basic cleaning (if necessary, based on the inspection)\n",
        "# For example, stripping whitespace from string columns\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object':\n",
        "        df[col] = df[col].str.strip()\n",
        "\n",
        "print(\"\\nDataFrame after basic cleaning:\")\n",
        "display(df.head())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of the DataFrame:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                   PMB Code  \\\n",
              "0  BRAIN AND NERVOUS SYSTEM   \n",
              "1                      906A   \n",
              "2                       NaN   \n",
              "3                       NaN   \n",
              "4                       NaN   \n",
              "\n",
              "                                     PMB Description  \\\n",
              "0                                                NaN   \n",
              "1  Acute generalised paralysis, including polio a...   \n",
              "2                                                NaN   \n",
              "3                                                NaN   \n",
              "4                                                NaN   \n",
              "\n",
              "                                 Treatment Component ICD10 Code  \\\n",
              "0                                                NaN        NaN   \n",
              "1  Medical management; ventilation and plasmapher...      A80.0   \n",
              "2                                                NaN      A80.1   \n",
              "3                                                NaN      A80.2   \n",
              "4                                                NaN      A80.3   \n",
              "\n",
              "                                  ICD10 Description  Comments  \n",
              "0                                                NaN      NaN  \n",
              "1  Acute paralytic poliomyelitis, vaccine-associated      NaN  \n",
              "2  Acute paralytic poliomyelitis, wild virus, imp...      NaN  \n",
              "3  Acute paralytic poliomyelitis, wild virus, ind...      NaN  \n",
              "4  Acute paralytic poliomyelitis, other and unspe...      NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e2cf6e87-6337-4b2c-8fa3-3afd2bc8fb57\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PMB Code</th>\n",
              "      <th>PMB Description</th>\n",
              "      <th>Treatment Component</th>\n",
              "      <th>ICD10 Code</th>\n",
              "      <th>ICD10 Description</th>\n",
              "      <th>Comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BRAIN AND NERVOUS SYSTEM</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>906A</td>\n",
              "      <td>Acute generalised paralysis, including polio a...</td>\n",
              "      <td>Medical management; ventilation and plasmapher...</td>\n",
              "      <td>A80.0</td>\n",
              "      <td>Acute paralytic poliomyelitis, vaccine-associated</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A80.1</td>\n",
              "      <td>Acute paralytic poliomyelitis, wild virus, imp...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A80.2</td>\n",
              "      <td>Acute paralytic poliomyelitis, wild virus, ind...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A80.3</td>\n",
              "      <td>Acute paralytic poliomyelitis, other and unspe...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e2cf6e87-6337-4b2c-8fa3-3afd2bc8fb57')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e2cf6e87-6337-4b2c-8fa3-3afd2bc8fb57 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e2cf6e87-6337-4b2c-8fa3-3afd2bc8fb57');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0735a020-f161-4489-92da-ddda0f845efe\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0735a020-f161-4489-92da-ddda0f845efe')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0735a020-f161-4489-92da-ddda0f845efe button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Missing values per column:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "PMB Code               1346\n",
              "PMB Description        1349\n",
              "Treatment Component    1349\n",
              "ICD10 Code                4\n",
              "ICD10 Description         3\n",
              "Comments               1360\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>PMB Code</th>\n",
              "      <td>1346</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PMB Description</th>\n",
              "      <td>1349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Treatment Component</th>\n",
              "      <td>1349</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ICD10 Code</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ICD10 Description</th>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Comments</th>\n",
              "      <td>1360</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Data types:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "PMB Code               object\n",
              "PMB Description        object\n",
              "Treatment Component    object\n",
              "ICD10 Code             object\n",
              "ICD10 Description      object\n",
              "Comments               object\n",
              "dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>PMB Code</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PMB Description</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Treatment Component</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ICD10 Code</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ICD10 Description</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Comments</th>\n",
              "      <td>object</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame after basic cleaning:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                   PMB Code  \\\n",
              "0  BRAIN AND NERVOUS SYSTEM   \n",
              "1                      906A   \n",
              "2                       NaN   \n",
              "3                       NaN   \n",
              "4                       NaN   \n",
              "\n",
              "                                     PMB Description  \\\n",
              "0                                                NaN   \n",
              "1  Acute generalised paralysis, including polio a...   \n",
              "2                                                NaN   \n",
              "3                                                NaN   \n",
              "4                                                NaN   \n",
              "\n",
              "                                 Treatment Component ICD10 Code  \\\n",
              "0                                                NaN        NaN   \n",
              "1  Medical management; ventilation and plasmapher...      A80.0   \n",
              "2                                                NaN      A80.1   \n",
              "3                                                NaN      A80.2   \n",
              "4                                                NaN      A80.3   \n",
              "\n",
              "                                  ICD10 Description  Comments  \n",
              "0                                                NaN      NaN  \n",
              "1  Acute paralytic poliomyelitis, vaccine-associated      NaN  \n",
              "2  Acute paralytic poliomyelitis, wild virus, imp...      NaN  \n",
              "3  Acute paralytic poliomyelitis, wild virus, ind...      NaN  \n",
              "4  Acute paralytic poliomyelitis, other and unspe...      NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-84e5a43d-4d59-4f2a-9bbd-a04d115417be\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PMB Code</th>\n",
              "      <th>PMB Description</th>\n",
              "      <th>Treatment Component</th>\n",
              "      <th>ICD10 Code</th>\n",
              "      <th>ICD10 Description</th>\n",
              "      <th>Comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BRAIN AND NERVOUS SYSTEM</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>906A</td>\n",
              "      <td>Acute generalised paralysis, including polio a...</td>\n",
              "      <td>Medical management; ventilation and plasmapher...</td>\n",
              "      <td>A80.0</td>\n",
              "      <td>Acute paralytic poliomyelitis, vaccine-associated</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A80.1</td>\n",
              "      <td>Acute paralytic poliomyelitis, wild virus, imp...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A80.2</td>\n",
              "      <td>Acute paralytic poliomyelitis, wild virus, ind...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A80.3</td>\n",
              "      <td>Acute paralytic poliomyelitis, other and unspe...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-84e5a43d-4d59-4f2a-9bbd-a04d115417be')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-84e5a43d-4d59-4f2a-9bbd-a04d115417be button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-84e5a43d-4d59-4f2a-9bbd-a04d115417be');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-7ab47f2d-a24d-4393-ade6-cfb68d026273\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7ab47f2d-a24d-4393-ade6-cfb68d026273')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-7ab47f2d-a24d-4393-ade6-cfb68d026273 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e210252"
      },
      "source": [
        "**Reasoning**:\n",
        "The data has been loaded successfully and basic cleaning has been performed. The next steps would involve handling missing values and potentially transforming columns relevant to the downstream tasks, but based on the subtask description and the high number of missing values in some columns, it seems the focus is on the 'ICD10 Code' and 'ICD10 Description' columns which have fewer missing values. For the purpose of this subtask (preprocessing for model use), removing rows with missing 'ICD10 Code' or 'ICD10 Description' is a reasonable approach.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "30d92323",
        "outputId": "3c5fbf1b-77ee-4660-8617-74a2694f9fab"
      },
      "source": [
        "# Drop rows where 'ICD10 Code' or 'ICD10 Description' are missing\n",
        "df.dropna(subset=['ICD10 Code', 'ICD10 Description'], inplace=True)\n",
        "\n",
        "# Display the number of remaining rows and missing values after dropping\n",
        "print(f\"\\nNumber of rows after dropping missing ICD10 codes/descriptions: {len(df)}\")\n",
        "print(\"\\nMissing values after dropping rows:\")\n",
        "display(df.isnull().sum())\n",
        "\n",
        "# Reset index after dropping rows\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(\"\\nDataFrame after dropping rows and resetting index:\")\n",
        "display(df.head())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "['ICD10 Description']",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3638517899.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Drop rows where 'ICD10 Code' or 'ICD10 Description' are missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ICD10 Code'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ICD10 Description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Display the number of remaining rows and missing values after dropping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nNumber of rows after dropping missing ICD10 codes/descriptions: {len(df)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdropna\u001b[0;34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[0m\n\u001b[1;32m   6668\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6669\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6670\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6671\u001b[0m             \u001b[0magg_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ['ICD10 Description']"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9df4216b"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because 'ICD10 Description' was not found as a column name. Looking at the previous output, it seems the column names are correct. This error is unexpected and might be due to an internal state issue. I will try the same command again, ensuring the column names are exactly as displayed in the output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "DFLhp_lb50DD",
        "outputId": "b94c283d-f0c7-4eeb-c2b3-cfdb81fce86b"
      },
      "source": [
        "# Drop rows where 'ICD10 Code' or 'ICD10 Description' are missing\n",
        "df.dropna(subset=['ICD10 Code', 'ICD10 Description'], inplace=True)\n",
        "\n",
        "# Display the number of remaining rows and missing values after dropping\n",
        "print(f\"\\nNumber of rows after dropping missing ICD10 codes/descriptions: {len(df)}\")\n",
        "print(\"\\nMissing values after dropping rows:\")\n",
        "display(df.isnull().sum())\n",
        "\n",
        "# Reset index after dropping rows\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(\"\\nDataFrame after dropping rows and resetting index:\")\n",
        "display(df.head())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "['ICD10 Description']",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3638517899.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Drop rows where 'ICD10 Code' or 'ICD10 Description' are missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ICD10 Code'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ICD10 Description'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Display the number of remaining rows and missing values after dropping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nNumber of rows after dropping missing ICD10 codes/descriptions: {len(df)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdropna\u001b[0;34m(self, axis, how, thresh, subset, inplace, ignore_index)\u001b[0m\n\u001b[1;32m   6668\u001b[0m             \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6669\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6670\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6671\u001b[0m             \u001b[0magg_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magg_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ['ICD10 Description']"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4a169bf"
      },
      "source": [
        "**Reasoning**:\n",
        "The `KeyError` persists even after retrying the command. This is very strange as the previous outputs clearly show 'ICD10 Description' as a column name. To understand what's happening, I will inspect the column names directly from the DataFrame to confirm their exact spelling and presence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "BU06edGg51pe",
        "outputId": "5fc6d784-b572-45a9-f201-c34551463877"
      },
      "source": [
        "# Display the column names of the DataFrame\n",
        "print(\"DataFrame columns:\")\n",
        "display(df.columns)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame columns:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Index(['PMB Code', 'PMB Description', 'Treatment Component', 'ICD10 Code',\n",
              "       'ICD10 Description ', 'Comments'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aab87af"
      },
      "source": [
        "**Reasoning**:\n",
        "The inspection of column names reveals a trailing space in 'ICD10 Description '. This was the cause of the `KeyError`. I will now retry dropping the rows with missing values using the correct column name, including the trailing space.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "id": "58c5a709",
        "outputId": "02390c65-e1fe-4889-8cdf-2a3f661f442b"
      },
      "source": [
        "# Drop rows where 'ICD10 Code' or 'ICD10 Description ' are missing (corrected column name)\n",
        "df.dropna(subset=['ICD10 Code', 'ICD10 Description '], inplace=True)\n",
        "\n",
        "# Display the number of remaining rows and missing values after dropping\n",
        "print(f\"\\nNumber of rows after dropping missing ICD10 codes/descriptions: {len(df)}\")\n",
        "print(\"\\nMissing values after dropping rows:\")\n",
        "display(df.isnull().sum())\n",
        "\n",
        "# Reset index after dropping rows\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(\"\\nDataFrame after dropping rows and resetting index:\")\n",
        "display(df.head())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of rows after dropping missing ICD10 codes/descriptions: 1396\n",
            "\n",
            "Missing values after dropping rows:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "PMB Code               1345\n",
              "PMB Description        1345\n",
              "Treatment Component    1345\n",
              "ICD10 Code                0\n",
              "ICD10 Description         0\n",
              "Comments               1356\n",
              "dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>PMB Code</th>\n",
              "      <td>1345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PMB Description</th>\n",
              "      <td>1345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Treatment Component</th>\n",
              "      <td>1345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ICD10 Code</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ICD10 Description</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Comments</th>\n",
              "      <td>1356</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataFrame after dropping rows and resetting index:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  PMB Code                                    PMB Description  \\\n",
              "0     906A  Acute generalised paralysis, including polio a...   \n",
              "1      NaN                                                NaN   \n",
              "2      NaN                                                NaN   \n",
              "3      NaN                                                NaN   \n",
              "4      NaN                                                NaN   \n",
              "\n",
              "                                 Treatment Component ICD10 Code  \\\n",
              "0  Medical management; ventilation and plasmapher...      A80.0   \n",
              "1                                                NaN      A80.1   \n",
              "2                                                NaN      A80.2   \n",
              "3                                                NaN      A80.3   \n",
              "4                                                NaN      A80.9   \n",
              "\n",
              "                                  ICD10 Description  Comments  \n",
              "0  Acute paralytic poliomyelitis, vaccine-associated      NaN  \n",
              "1  Acute paralytic poliomyelitis, wild virus, imp...      NaN  \n",
              "2  Acute paralytic poliomyelitis, wild virus, ind...      NaN  \n",
              "3  Acute paralytic poliomyelitis, other and unspe...      NaN  \n",
              "4                   Acute poliomyelitis, unspecified      NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0f15f945-bfad-41de-bbcd-d2284b501722\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PMB Code</th>\n",
              "      <th>PMB Description</th>\n",
              "      <th>Treatment Component</th>\n",
              "      <th>ICD10 Code</th>\n",
              "      <th>ICD10 Description</th>\n",
              "      <th>Comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>906A</td>\n",
              "      <td>Acute generalised paralysis, including polio a...</td>\n",
              "      <td>Medical management; ventilation and plasmapher...</td>\n",
              "      <td>A80.0</td>\n",
              "      <td>Acute paralytic poliomyelitis, vaccine-associated</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A80.1</td>\n",
              "      <td>Acute paralytic poliomyelitis, wild virus, imp...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A80.2</td>\n",
              "      <td>Acute paralytic poliomyelitis, wild virus, ind...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A80.3</td>\n",
              "      <td>Acute paralytic poliomyelitis, other and unspe...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A80.9</td>\n",
              "      <td>Acute poliomyelitis, unspecified</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0f15f945-bfad-41de-bbcd-d2284b501722')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0f15f945-bfad-41de-bbcd-d2284b501722 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0f15f945-bfad-41de-bbcd-d2284b501722');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-abc6ff62-8d60-443a-b4cf-bc6e1a119c9f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-abc6ff62-8d60-443a-b4cf-bc6e1a119c9f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-abc6ff62-8d60-443a-b4cf-bc6e1a119c9f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b069e729"
      },
      "source": [
        "## Implement clinicalbert for note analysis\n",
        "\n",
        "### Subtask:\n",
        "Set up ClinicalBERT to analyze specialist notes and extract relevant medical information like symptoms and diagnoses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37ef8577"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary libraries and modules from the Hugging Face `transformers` library and load a pre-trained ClinicalBERT model and its corresponding tokenizer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294,
          "referenced_widgets": [
            "b3b4f92d33404aebab4613f48d583c7c",
            "0f9a9ee610da4a2c9145e655e9dc9f7e",
            "9a39efbdf17b4f80af0f6b5747aabf04",
            "29f360a92c3f4994b531415ffcb899c9",
            "5acf4420bf434ceaa312da833f16bfad",
            "61950a467c5f404e86065e3fb65b337a",
            "36c64f260b1b4e6d8a1f64346107db5b",
            "4bd26d61fc864809a4519d5f26124bfd",
            "dd26a642341d48119ca0ebf372d61bc3",
            "393d59139b2f4a2aafaca8935105913c",
            "e2061a2ccdb84ababf9b0b4b3d5d1c73",
            "296c16cc3e83449f84fab669ec4dbc39",
            "511815e9a1824227b5e6ad7d53c55db4",
            "1ffe93b911a649508cef813fd3c23b0a",
            "f5d5790700b1411088167dc26386a783",
            "184de08451714f43a74a7614f78c891e",
            "d8ac571d2f424acf988bb0913489d3be",
            "886af8372aea4bc284a30f322e103daf",
            "863bd357904a40169b8278630586cde6",
            "8078fbc294cc4507874888c27ccb5492",
            "acf417c9c240435a983a26897edd1c1a",
            "885eb29b111949fa881b39f8b85ca7d4",
            "ecc845b8a59d4ce48a0efa94ef383c7d",
            "8e17c0e8a3714be2905a3292d8994867",
            "9d6b9f7c006a41b1a0bf02dad1de026e",
            "f77adc1a14a348bc8386216257050b95",
            "bb3fc67a084543de84a11290da38d2c0",
            "2d43fd048c9f4eecadf73f825845b2d5",
            "48026425ab8e473a9e0febd53e78f810",
            "809b84fb5d804893b4f4b5bd3ec432f4",
            "54338b94319f4d01938c9fd7e6855893",
            "27a546ff1446477dac77857e81d64d32",
            "a64db286f1d446aeb1982d3ff5dc8ed3"
          ]
        },
        "id": "999a74d1",
        "outputId": "11c9605a-7a99-4784-a41b-ebfc8b7f52f8"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load a pre-trained ClinicalBERT model and tokenizer\n",
        "model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Tokenizer and model loaded successfully: {model_name}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3b4f92d33404aebab4613f48d583c7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "296c16cc3e83449f84fab669ec4dbc39"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ecc845b8a59d4ce48a0efa94ef383c7d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer and model loaded successfully: emilyalsentzer/Bio_ClinicalBERT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ebe087f"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a function that takes specialist notes as input, tokenizes them using the loaded tokenizer, and passes them through the ClinicalBERT model to get the output embeddings as features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US2iUhXS6G6p"
      },
      "source": [
        "import torch\n",
        "\n",
        "def analyze_specialist_notes(notes):\n",
        "    \"\"\"\n",
        "    Analyzes specialist notes using ClinicalBERT to extract features.\n",
        "\n",
        "    Args:\n",
        "        notes (str): The specialist notes as a string.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The hidden states (features) from the last layer of ClinicalBERT.\n",
        "    \"\"\"\n",
        "    # Tokenize the input notes\n",
        "    encoded_input = tokenizer(notes, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # Pass the tokenized input through the model\n",
        "    with torch.no_grad():  # Disable gradient calculation for inference\n",
        "        output = model(**encoded_input, output_hidden_states=True)\n",
        "\n",
        "    # Extract the hidden states from the last layer\n",
        "    # output.hidden_states is a tuple of hidden states for each layer\n",
        "    # We take the last element which corresponds to the last layer\n",
        "    last_hidden_states = output.hidden_states[-1]\n",
        "\n",
        "    # You might want to take the mean of the hidden states across tokens\n",
        "    # or specifically the [CLS] token representation depending on the downstream task\n",
        "    # For this example, we'll return the tensor for all tokens\n",
        "    # A common approach for sentence representation is the [CLS] token embedding:\n",
        "    # cls_embedding = last_hidden_states[:, 0, :]\n",
        "    # Or taking the mean of all tokens (excluding padding):\n",
        "    # mean_embedding = last_hidden_states.mean(dim=1)\n",
        "\n",
        "    # For now, we'll just return the raw last hidden states tensor\n",
        "    return last_hidden_states\n",
        "\n",
        "# Example usage (optional, for testing the function)\n",
        "# sample_note = \"Patient presented with severe headache and fever, suspected meningitis.\"\n",
        "# features = analyze_specialist_notes(sample_note)\n",
        "# print(\"Shape of extracted features:\", features.shape)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cd1391d"
      },
      "source": [
        "## Train icd-10 code prediction model\n",
        "\n",
        "### Subtask:\n",
        "Train a model using the processed data and the output from ClinicalBERT to predict the most accurate ICD-10 codes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06bceac1"
      },
      "source": [
        "**Reasoning**:\n",
        "Prepare the input data for training by processing specialist notes (simulated here as there are no actual notes) using the `analyze_specialist_notes` function and pairing the resulting features with the corresponding 'ICD10 Code' from the `df` DataFrame. Encode the 'ICD10 Code' into numerical labels and split the data into training and testing sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "2a4efa17",
        "outputId": "1910bc70-8d51-4f43-8355-77eb269b3c8a"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Since we don't have actual specialist notes, we will simulate the input data.\n",
        "# We will use the 'ICD10 Description ' column as a proxy for specialist notes for demonstration purposes.\n",
        "# In a real scenario, you would replace this with your actual specialist notes data.\n",
        "\n",
        "# Create a list of \"simulated\" specialist notes from the ICD10 Description column\n",
        "simulated_notes = df['ICD10 Description '].tolist()\n",
        "\n",
        "# Process the simulated notes using the analyze_specialist_notes function\n",
        "# Note: This assumes analyze_specialist_notes returns a consistent feature vector shape.\n",
        "# If it returns varying shapes (e.g., per token), you might need to aggregate.\n",
        "# For simplicity, let's assume it returns a fixed-size representation per note (e.g., [CLS] token or mean pooling).\n",
        "# We will simulate this fixed-size output for now as analyze_specialist_notes is defined to return hidden states.\n",
        "# A more realistic approach for classification would be to use the [CLS] token embedding or mean pooling.\n",
        "# Let's modify analyze_specialist_notes or process its output to get a single vector per note.\n",
        "\n",
        "# Let's redefine or adapt analyze_specialist_notes to return a single vector per note.\n",
        "# Assuming the original analyze_specialist_notes is available and works,\n",
        "# we will process its output to get a single vector per note, e.g., by taking the mean.\n",
        "\n",
        "def analyze_specialist_notes_vector(notes):\n",
        "    \"\"\"\n",
        "    Analyzes specialist notes using ClinicalBERT and returns a single feature vector.\n",
        "    \"\"\"\n",
        "    hidden_states = analyze_specialist_notes(notes)\n",
        "    # Take the mean of the token embeddings to get a single vector representation for the note\n",
        "    # This is a common approach for sentence/document embedding\n",
        "    if hidden_states is not None:\n",
        "        # Ensure we are averaging across tokens (dim=1) and handling potential padding\n",
        "        # Assuming padding tokens were handled during tokenization and are not included in meaningful hidden states\n",
        "        # If padding is present, you might need to use attention masks to average only over non-padding tokens.\n",
        "        # For simplicity here, we average over all tokens.\n",
        "        return hidden_states.mean(dim=1).squeeze().numpy() # Squeeze to remove batch dimension, convert to numpy\n",
        "    else:\n",
        "        return None # Handle cases where analysis fails\n",
        "\n",
        "# Apply the function to get features for each simulated note\n",
        "# This might take some time depending on the number of notes and GPU availability\n",
        "note_features = [analyze_specialist_notes_vector(note) for note in simulated_notes]\n",
        "\n",
        "# Filter out any notes that failed analysis (returned None) and their corresponding labels\n",
        "valid_indices = [i for i, features in enumerate(note_features) if features is not None]\n",
        "note_features = np.array([note_features[i] for i in valid_indices])\n",
        "icd10_codes = df['ICD10 Code'].iloc[valid_indices].tolist()\n",
        "\n",
        "\n",
        "# Encode the 'ICD10 Code' column into numerical labels\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(icd10_codes)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "# Features are `note_features`, Labels are `encoded_labels`\n",
        "X_train, X_test, y_train, y_test = train_test_split(note_features, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels)\n",
        "\n",
        "print(\"Shape of training features:\", X_train.shape)\n",
        "print(\"Shape of testing features:\", X_test.shape)\n",
        "print(\"Shape of training labels:\", y_train.shape)\n",
        "print(\"Shape of testing labels:\", y_test.shape)\n",
        "print(\"Number of unique ICD-10 codes:\", len(label_encoder.classes_))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1829871914.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Split the data into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Features are `note_features`, Labels are `encoded_labels`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnote_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoded_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape of training features:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2870\u001b[0m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCVClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2872\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2874\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_common_namespace_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1907\u001b[0m         \"\"\"\n\u001b[1;32m   1908\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1909\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1910\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_iter_indices\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2316\u001b[0m         \u001b[0mclass_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_counts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2318\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2319\u001b[0m                 \u001b[0;34m\"The least populated class in y has only 1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2320\u001b[0m                 \u001b[0;34m\" member, which is too few. The minimum\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b72be707"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step failed because using `stratify` in `train_test_split` requires that each class has at least two members in the dataset to be split. The error indicates that some ICD-10 codes appear only once in the filtered data. To address this, we need to remove classes with only one instance before splitting the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "IXlwnylu61Rh",
        "outputId": "ac5a4696-8375-4e49-deed-4192e6af50af"
      },
      "source": [
        "# Identify classes with only one instance\n",
        "unique_classes, counts = np.unique(encoded_labels, return_counts=True)\n",
        "classes_to_remove = unique_classes[counts < 2]\n",
        "\n",
        "# Find indices corresponding to instances of these classes\n",
        "indices_to_remove = [i for i, label in enumerate(encoded_labels) if label in classes_to_remove]\n",
        "\n",
        "# Filter out these instances from features and labels\n",
        "filtered_note_features = np.delete(note_features, indices_to_remove, axis=0)\n",
        "filtered_encoded_labels = np.delete(encoded_labels, indices_to_remove)\n",
        "\n",
        "# Verify that all remaining classes have at least two instances\n",
        "unique_classes_filtered, counts_filtered = np.unique(filtered_encoded_labels, return_counts=True)\n",
        "print(f\"Number of classes with less than 2 instances after filtering: {np.sum(counts_filtered < 2)}\")\n",
        "\n",
        "\n",
        "# Now, split the filtered data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(filtered_note_features, filtered_encoded_labels, test_size=0.2, random_state=42, stratify=filtered_encoded_labels)\n",
        "\n",
        "print(\"Shape of training features after filtering and split:\", X_train.shape)\n",
        "print(\"Shape of testing features after filtering and split:\", X_test.shape)\n",
        "print(\"Shape of training labels after filtering and split:\", y_train.shape)\n",
        "print(\"Shape of testing labels after filtering and split:\", y_test.shape)\n",
        "print(\"Number of unique ICD-10 codes after filtering:\", len(np.unique(filtered_encoded_labels)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes with less than 2 instances after filtering: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The test_size = 14 should be greater or equal to the number of classes = 33",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-34055805.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Now, split the filtered data into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_note_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_encoded_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiltered_encoded_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape of training features after filtering and split:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2870\u001b[0m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCVClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2872\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2874\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_common_namespace_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1907\u001b[0m         \"\"\"\n\u001b[1;32m   1908\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1909\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1910\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_iter_indices\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2329\u001b[0m             )\n\u001b[1;32m   2330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_test\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2331\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2332\u001b[0m                 \u001b[0;34m\"The test_size = %d should be greater or \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2333\u001b[0m                 \u001b[0;34m\"equal to the number of classes = %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The test_size = 14 should be greater or equal to the number of classes = 33"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eb82527"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step failed because the calculated test set size (14 instances) is smaller than the number of unique classes (33). This is a limitation of `stratify` in `train_test_split` when the dataset is small relative to the number of classes. To resolve this, we need to either increase the `test_size` or use a different splitting strategy if a larger test set is not feasible. Given the constraints, increasing the test size is the most straightforward way to satisfy the stratification requirement. Let's adjust the `test_size` to ensure there are at least as many test instances as there are classes, while still maintaining a reasonable proportion for training. A test size of 0.5 should allocate roughly half the data to the test set, which should be sufficient.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm3gRjDc63PK",
        "outputId": "8174e39b-8304-4f05-d11c-25310ae3c2d4"
      },
      "source": [
        "# Split the filtered data into training and testing sets\n",
        "# Increase test_size to ensure n_test >= n_classes for stratification\n",
        "X_train, X_test, y_train, y_test = train_test_split(filtered_note_features, filtered_encoded_labels, test_size=0.5, random_state=42, stratify=filtered_encoded_labels)\n",
        "\n",
        "print(\"Shape of training features after filtering and split (with adjusted test_size):\", X_train.shape)\n",
        "print(\"Shape of testing features after filtering and split (with adjusted test_size):\", X_test.shape)\n",
        "print(\"Shape of training labels after filtering and split (with adjusted test_size):\", y_train.shape)\n",
        "print(\"Shape of testing labels after filtering and split (with adjusted test_size):\", y_test.shape)\n",
        "print(\"Number of unique ICD-10 codes after filtering:\", len(np.unique(filtered_encoded_labels)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of training features after filtering and split (with adjusted test_size): (33, 768)\n",
            "Shape of testing features after filtering and split (with adjusted test_size): (33, 768)\n",
            "Shape of training labels after filtering and split (with adjusted test_size): (33,)\n",
            "Shape of testing labels after filtering and split (with adjusted test_size): (33,)\n",
            "Number of unique ICD-10 codes after filtering: 33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edc7f42d"
      },
      "source": [
        "**Reasoning**:\n",
        "The data has been prepared and split. Now, choose a suitable classification model for multi-class classification, train it on the training data, and evaluate its performance on the testing data using appropriate metrics. A Logistic Regression classifier is a good starting point for multi-class classification with numerical features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d312790c",
        "outputId": "e6eed891-fa03-4455-827c-a03e23e93498"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "import warnings\n",
        "\n",
        "# Suppress ConvergenceWarning for Logistic Regression\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "# Choose a classification model\n",
        "# Logistic Regression is a good baseline for multi-class classification\n",
        "model = LogisticRegression(max_iter=1000, solver='liblinear') # Increased max_iter and used liblinear for multi-class on potentially small datasets\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the testing data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "# For precision, recall, and F1-score in multi-class, use 'weighted' or 'macro' average\n",
        "# 'weighted' accounts for class imbalance\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Model Precision (weighted): {precision:.4f}\")\n",
        "print(f\"Model Recall (weighted): {recall:.4f}\")\n",
        "print(f\"Model F1-Score (weighted): {f1:.4f}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.8485\n",
            "Model Precision (weighted): 0.8333\n",
            "Model Recall (weighted): 0.8485\n",
            "Model F1-Score (weighted): 0.8384\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b6174d4"
      },
      "source": [
        "## Pair icd-10 codes with pmb information\n",
        "\n",
        "### Subtask:\n",
        "Develop logic to pair the predicted ICD-10 codes with the corresponding PMB information from the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bf6bd78"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a Python function `pair_icd10_with_pmb` that takes a predicted ICD-10 code, searches the original `df` DataFrame for matching rows, extracts PMB information ('PMB Code', 'PMB Description'), and handles cases with no matches or multiple matches, including missing PMB data within matching rows.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7e77eda"
      },
      "source": [
        "def pair_icd10_with_pmb(predicted_icd10_code, dataframe):\n",
        "    \"\"\"\n",
        "    Pairs a predicted ICD-10 code with corresponding PMB information from a DataFrame.\n",
        "\n",
        "    Args:\n",
        "        predicted_icd10_code (str): The ICD-10 code predicted by the model.\n",
        "        dataframe (pd.DataFrame): The DataFrame containing ICD-10 and PMB information.\n",
        "\n",
        "    Returns:\n",
        "        list or str: A list of dictionaries, where each dictionary contains the\n",
        "                     'PMB Code' and 'PMB Description' for a matching row.\n",
        "                     Returns a string message if no match is found.\n",
        "    \"\"\"\n",
        "    # Ensure the input is treated as a string for comparison\n",
        "    predicted_icd10_code_str = str(predicted_icd10_code)\n",
        "\n",
        "    # Search the DataFrame for rows where 'ICD10 Code' matches the predicted code\n",
        "    # Ensure comparison is done with strings if 'ICD10 Code' might have mixed types\n",
        "    matching_rows = dataframe[dataframe['ICD10 Code'].astype(str) == predicted_icd10_code_str]\n",
        "\n",
        "    if matching_rows.empty:\n",
        "        return f\"No PMB information found for ICD-10 code: {predicted_icd10_code}\"\n",
        "    else:\n",
        "        # Extract 'PMB Code' and 'PMB Description' from matching rows\n",
        "        # Handle potential missing values by filling them or converting to string 'N/A'\n",
        "        pmb_info = []\n",
        "        for index, row in matching_rows.iterrows():\n",
        "            pmb_code = row['PMB Code'] if pd.notna(row['PMB Code']) else 'N/A'\n",
        "            pmb_description = row['PMB Description'] if pd.notna(row['PMB Description']) else 'N/A'\n",
        "            pmb_info.append({\n",
        "                'PMB Code': pmb_code,\n",
        "                'PMB Description': pmb_description\n",
        "            })\n",
        "        return pmb_info\n",
        "\n",
        "# Example usage (optional, for testing the function)\n",
        "# Assuming 'df' is your loaded and cleaned DataFrame\n",
        "# Test with a code known to exist (e.g., A80.0)\n",
        "# result_existing = pair_icd10_with_pmb('A80.0', df)\n",
        "# print(\"\\nResult for existing code 'A80.0':\")\n",
        "# print(result_existing)\n",
        "\n",
        "# Test with a code that might have multiple entries or missing PMB info if applicable in your df\n",
        "# result_multiple = pair_icd10_with_pmb('S27.01', df) # Example, check your df for codes with multiple entries\n",
        "# print(\"\\nResult for code 'S27.01' (or similar with multiple entries):\")\n",
        "# print(result_multiple)\n",
        "\n",
        "# Test with a code not expected to exist\n",
        "# result_non_existing = pair_icd10_with_pmb('Z99.9', df)\n",
        "# print(\"\\nResult for non-existing code 'Z99.9':\")\n",
        "# print(result_non_existing)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fff9dab"
      },
      "source": [
        "**Reasoning**:\n",
        "Test the created `pair_icd10_with_pmb` function with example ICD-10 codes to verify its functionality, including cases with existing codes (single and potentially multiple matches) and a non-existing code, and display the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4964f07d",
        "outputId": "08084bc5-e8b0-4c1d-acf5-b3be859f0821"
      },
      "source": [
        "# Test with a code known to exist (e.g., A80.0)\n",
        "result_existing = pair_icd10_with_pmb('A80.0', df)\n",
        "print(\"\\nResult for existing code 'A80.0':\")\n",
        "print(result_existing)\n",
        "\n",
        "# Test with a code that might have multiple entries or missing PMB info if applicable in your df\n",
        "# Check the unique values in 'ICD10 Code' and their counts to find codes with multiple entries\n",
        "# For example, let's pick one from the head that appears more than once if any, or just pick a random one\n",
        "# Let's test with 'A80.9' which appears multiple times in the head of the df\n",
        "result_multiple = pair_icd10_with_pmb('A80.9', df)\n",
        "print(\"\\nResult for code 'A80.9' (or similar with multiple entries):\")\n",
        "print(result_multiple)\n",
        "\n",
        "\n",
        "# Test with a code not expected to exist\n",
        "result_non_existing = pair_icd10_with_pmb('Z99.9', df)\n",
        "print(\"\\nResult for non-existing code 'Z99.9':\")\n",
        "print(result_non_existing)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Result for existing code 'A80.0':\n",
            "[{'PMB Code': '906A', 'PMB Description': 'Acute generalised paralysis, including polio and Guillain-Barre'}]\n",
            "\n",
            "Result for code 'A80.9' (or similar with multiple entries):\n",
            "[{'PMB Code': 'N/A', 'PMB Description': 'N/A'}]\n",
            "\n",
            "Result for non-existing code 'Z99.9':\n",
            "No PMB information found for ICD-10 code: Z99.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dac2877a"
      },
      "source": [
        "## Enhance code differentiation\n",
        "\n",
        "### Subtask:\n",
        "Implement techniques, possibly within or alongside the ClinicalBERT analysis, to improve the model's ability to differentiate between similar ICD-10 codes based on context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d829f68"
      },
      "source": [
        "**Reasoning**:\n",
        "Review the existing `analyze_specialist_notes` function and consider strategies for improving differentiation of similar ICD-10 codes based on context, potentially incorporating additional features or leveraging ICD-10 descriptions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e588cec"
      },
      "source": [
        "# Review of the existing analyze_specialist_notes function:\n",
        "# The function uses ClinicalBERT to get the last layer hidden states for the input notes.\n",
        "# This provides a rich contextual representation of the text.\n",
        "# The shape of the output is (batch_size, sequence_length, hidden_size),\n",
        "# where sequence_length is the number of tokens after tokenization and padding.\n",
        "# Currently, the downstream model (Logistic Regression) uses a single vector\n",
        "# representation per note (mean of token embeddings).\n",
        "\n",
        "# Strategies to improve differentiation between similar ICD-10 codes:\n",
        "\n",
        "# 1. Fine-tuning ClinicalBERT:\n",
        "# If a labeled dataset of specialist notes with ground truth ICD-10 codes were available,\n",
        "# fine-tuning the ClinicalBERT model on this dataset would adapt its representations\n",
        "# specifically for this task, potentially improving its ability to capture nuances\n",
        "# relevant to code differentiation. This would involve adding a classification head\n",
        "# on top of ClinicalBERT and training the entire model end-to-end.\n",
        "# Limitation: A suitable labeled dataset is required, which is not currently available.\n",
        "\n",
        "# 2. Feature Extraction:\n",
        "# Instead of just using the mean of token embeddings, explore other ways to aggregate\n",
        "# or utilize the last_hidden_states tensor.\n",
        "# - Using the [CLS] token embedding: This token is specifically designed to represent\n",
        "#   the entire sequence.\n",
        "# - Using attention mechanisms: If a more complex model is used downstream, attention\n",
        "#   could focus on specific parts of the notes that are most indicative of the correct\n",
        "#   ICD-10 code, helping differentiate similar codes.\n",
        "# - Extracting domain-specific terms/patterns: Use techniques like named entity recognition (NER)\n",
        "#   or keyword extraction to identify key medical terms, symptoms, or procedures that\n",
        "#   are crucial for distinguishing between similar conditions. These could be used as\n",
        "#   additional features alongside the BERT embeddings.\n",
        "\n",
        "# 3. Incorporating ICD-10 descriptions:\n",
        "# The 'ICD10 Description' column in the dataset provides valuable context for each code.\n",
        "# - Embed the ICD-10 descriptions: Use a language model (like ClinicalBERT itself or a\n",
        "#   separate one) to generate embeddings for the ICD-10 descriptions.\n",
        "# - Use description embeddings in the prediction model: The description embeddings could\n",
        "#   be used in several ways:\n",
        "#     a) As additional input features to the downstream classification model, combined\n",
        "#        with the note embeddings.\n",
        "#     b) In a more advanced architecture, compare the embedding of the specialist note\n",
        "#        to the embeddings of all possible ICD-10 descriptions. The code with the most\n",
        "#        similar description embedding could be a candidate for prediction. This is\n",
        "#        particularly useful for differentiating similar codes as it leverages the\n",
        "#        semantic meaning of the descriptions.\n",
        "\n",
        "# Potential Modifications to the feature extraction/downstream model:\n",
        "\n",
        "# - Modified Feature Extraction:\n",
        "#   - Change the `analyze_specialist_notes` function to return the [CLS] token embedding\n",
        "#     instead of the full sequence tensor or mean embedding.\n",
        "#   - Develop a separate function to extract domain-specific keywords or entities\n",
        "#     and combine these with the BERT embeddings.\n",
        "\n",
        "# - Modified Downstream Prediction Model:\n",
        "#   - Instead of a simple Logistic Regression, consider models like:\n",
        "#     - A neural network that can take combined features (BERT embeddings + domain terms).\n",
        "#     - A Siamese network-like architecture that compares the note embedding to\n",
        "#       description embeddings.\n",
        "#   - Implement techniques like:\n",
        "#     - Loss functions that penalize confusion between similar codes more heavily.\n",
        "#     - Incorporating similarity measures between ICD-10 codes (based on their descriptions\n",
        "#       or hierarchical structure) into the model training or post-processing.\n",
        "\n",
        "# For the immediate next steps within this subtask, focusing on leveraging the\n",
        "# ICD-10 descriptions as additional features alongside the ClinicalBERT embeddings\n",
        "# seems like a feasible approach given the available data.\n",
        "\n",
        "# Outline of the next step:\n",
        "# - Modify the feature generation process to incorporate embeddings of the ICD-10 descriptions.\n",
        "# - This will involve processing the 'ICD10 Description' column using the ClinicalBERT tokenizer and model\n",
        "#   to get embeddings for each description.\n",
        "# - These description embeddings will then be combined with the note embeddings for the\n",
        "#   corresponding ICD-10 codes in the filtered dataset.\n",
        "# - The downstream model will need to be updated to handle these combined features."
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84ef96bb"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate embeddings for the ICD-10 descriptions using ClinicalBERT and combine them with the note features for the corresponding ICD-10 codes in the filtered dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "d5ac3a40",
        "outputId": "59cc14b9-9aa6-41ca-c92b-cfadd45db517"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming 'df' is the loaded and cleaned DataFrame with 'ICD10 Code' and 'ICD10 Description '\n",
        "# Assuming 'filtered_note_features' contains the ClinicalBERT embeddings for the notes\n",
        "# corresponding to the filtered data in 'filtered_df'\n",
        "# Assuming 'filtered_encoded_labels' contains the encoded labels for the filtered data\n",
        "\n",
        "# Create a mapping from encoded label back to original ICD10 Code\n",
        "# This mapping is needed to find the corresponding ICD10 Description for each filtered note feature\n",
        "label_to_code = {label: code for label, code in zip(filtered_encoded_labels, filtered_df['ICD10 Code'])}\n",
        "\n",
        "# Get unique ICD-10 codes from the filtered data\n",
        "unique_icd10_codes = filtered_df['ICD10 Code'].unique().tolist()\n",
        "\n",
        "# Create a dictionary mapping ICD10 Code to its Description (handle potential duplicates by taking the first description)\n",
        "code_to_description = filtered_df.set_index('ICD10 Code')['ICD10 Description '].to_dict()\n",
        "\n",
        "# Generate embeddings for each unique ICD-10 description\n",
        "description_embeddings = {}\n",
        "for code in unique_icd10_codes:\n",
        "    description = code_to_description.get(code, \"\") # Get description, default to empty string if not found\n",
        "    if description: # Only process if description is not empty\n",
        "        # Use the analyze_specialist_notes function (or a slightly modified version for descriptions)\n",
        "        # Since analyze_specialist_notes returns the full sequence tensor, let's modify it\n",
        "        # or process the output to get a single vector representation (e.g., mean or CLS)\n",
        "        # For simplicity, let's take the mean of the token embeddings for the description\n",
        "        encoded_description = tokenizer(description, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            output = model(**encoded_description, output_hidden_states=True)\n",
        "            last_hidden_states = output.hidden_hidden_states[-1]\n",
        "            # Take the mean of token embeddings as the description embedding\n",
        "            mean_description_embedding = last_hidden_states.mean(dim=1).squeeze().numpy() # Convert to numpy\n",
        "            description_embeddings[code] = mean_description_embedding\n",
        "    else:\n",
        "         # Handle cases where description is missing even after filtering (shouldn't happen based on previous steps, but as a safeguard)\n",
        "         description_embeddings[code] = np.zeros(model.config.hidden_size) # Use a zero vector for missing descriptions\n",
        "\n",
        "\n",
        "# Now, combine the note features with the corresponding description embeddings\n",
        "# We need to align the note features with the correct description embedding based on the ICD10 code\n",
        "combined_features = []\n",
        "for i, note_feature in enumerate(filtered_note_features):\n",
        "    # Get the original ICD10 code for this note feature\n",
        "    # Use the index 'i' which corresponds to the index in filtered_df after processing\n",
        "    original_icd10_code = filtered_df.iloc[i]['ICD10 Code']\n",
        "\n",
        "    # Get the corresponding description embedding\n",
        "    desc_embedding = description_embeddings.get(original_icd10_code)\n",
        "\n",
        "    if desc_embedding is not None:\n",
        "        # Concatenate note feature (mean embedding) and description embedding\n",
        "        # Assuming filtered_note_features are already mean embeddings (shape: (num_samples, hidden_size))\n",
        "        # If filtered_note_features is still (num_samples, seq_len, hidden_size), we need to get mean here\n",
        "        if len(note_feature.shape) > 1: # Check if it's not already a single vector\n",
        "            note_feature_vec = note_feature.mean(axis=0) # Take mean across sequence length\n",
        "        else:\n",
        "            note_feature_vec = note_feature # It's already a single vector\n",
        "\n",
        "        combined_feature_vec = np.concatenate((note_feature_vec, desc_embedding))\n",
        "        combined_features.append(combined_feature_vec)\n",
        "    else:\n",
        "        # This case should ideally not happen if filtering was done correctly\n",
        "        print(f\"Warning: No description embedding found for code {original_icd10_code} at index {i}\")\n",
        "        # Decide how to handle this - skip, use zero vector, etc.\n",
        "        # For now, let's skip this sample\n",
        "        pass # Or append np.concatenate((note_feature_vec, np.zeros(model.config.hidden_size)))\n",
        "\n",
        "\n",
        "combined_features = np.array(combined_features)\n",
        "\n",
        "print(\"Shape of combined features (note embeddings + description embeddings):\", combined_features.shape)\n",
        "\n",
        "# The combined_features can now be used to train the downstream prediction model\n",
        "# Need to ensure the corresponding labels are also filtered/aligned if any samples were skipped\n",
        "# Based on the logic, samples are only skipped if description embedding is missing, which shouldn't happen.\n",
        "# So the labels should align with combined_features if we iterate through filtered_df\n",
        "\n",
        "# Re-align labels just to be safe, using the indices from the combined_features list\n",
        "# This assumes that if a sample was skipped, its corresponding label is also excluded\n",
        "# A more robust way is to rebuild the filtered_encoded_labels based on the indices of combined_features\n",
        "# Let's use the indices from filtered_df that successfully generated combined features\n",
        "successful_indices = [i for i, note_feature in enumerate(filtered_note_features) if filtered_df.iloc[i]['ICD10 Code'] in description_embeddings]\n",
        "aligned_labels = filtered_encoded_labels[successful_indices]\n",
        "\n",
        "print(\"Shape of aligned labels:\", aligned_labels.shape)\n",
        "\n",
        "# Update filtered_note_features and filtered_encoded_labels to the combined and aligned versions\n",
        "# This assumes the next step will use these variables\n",
        "filtered_note_features_combined = combined_features\n",
        "filtered_encoded_labels_aligned = aligned_labels\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'filtered_df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-816704173.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Create a mapping from encoded label back to original ICD10 Code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# This mapping is needed to find the corresponding ICD10 Description for each filtered note feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mlabel_to_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_encoded_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ICD10 Code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Get unique ICD-10 codes from the filtered data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'filtered_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a4260cf"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `filtered_df` was not defined. This dataframe was created in a previous subtask but was not preserved. I need to recreate `filtered_df` by applying the same filtering steps used before to the original `df`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "SCovrhgb7NRN",
        "outputId": "752f0e1b-1c1d-44c5-cee6-5ee0e994dbb6"
      },
      "source": [
        "# Recreate filtered_df by applying the filtering steps from the previous subtask\n",
        "# Filter out rows where the ICD-10 code count is less than or equal to 1\n",
        "icd10_counts = df['ICD10 Code'].value_counts()\n",
        "codes_to_keep = icd10_counts[icd10_counts > 1].index\n",
        "filtered_df = df[df['ICD10 Code'].isin(codes_to_keep)].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "print(\"Shape of recreated filtered_df:\", filtered_df.shape)\n",
        "\n",
        "# Now, regenerate filtered_note_features and filtered_encoded_labels based on the recreated filtered_df\n",
        "# This step repeats part of the logic from the 'Train icd-10 code prediction model' subtask,\n",
        "# but is necessary to align features and labels with the recreated filtered_df.\n",
        "\n",
        "# Simulate specialist notes from the 'ICD10 Description ' column of the filtered_df\n",
        "filtered_notes = filtered_df['ICD10 Description '].tolist()\n",
        "\n",
        "# Process the simulated notes using the modified analyze_specialist_notes function\n",
        "# that returns a single feature vector (mean embedding) per note\n",
        "filtered_note_features = []\n",
        "for note in filtered_notes:\n",
        "    encoded_input = tokenizer(note, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        output = model(**encoded_input, output_hidden_states=True)\n",
        "        last_hidden_states = output.hidden_states[-1]\n",
        "        # Take the mean of token embeddings as the note embedding\n",
        "        mean_note_embedding = last_hidden_states.mean(dim=1).squeeze().numpy() # Convert to numpy\n",
        "        filtered_note_features.append(mean_note_embedding)\n",
        "\n",
        "filtered_note_features = np.array(filtered_note_features)\n",
        "\n",
        "print(\"Shape of regenerated filtered_note_features:\", filtered_note_features.shape)\n",
        "\n",
        "# Encode the 'ICD10 Code' from the filtered_df into numerical labels\n",
        "# Use LabelEncoder to ensure consistency with previous steps\n",
        "label_encoder = LabelEncoder()\n",
        "filtered_encoded_labels = label_encoder.fit_transform(filtered_df['ICD10 Code'])\n",
        "\n",
        "print(\"Shape of regenerated filtered_encoded_labels:\", filtered_encoded_labels.shape)\n",
        "\n",
        "# Now retry the logic from the previous failed command to generate description embeddings\n",
        "# and combine them with the note features.\n",
        "\n",
        "# Create a dictionary mapping ICD10 Code to its Description (handle potential duplicates by taking the first description)\n",
        "code_to_description = filtered_df.set_index('ICD10 Code')['ICD10 Description '].to_dict()\n",
        "\n",
        "# Get unique ICD-10 codes from the filtered data\n",
        "unique_icd10_codes = filtered_df['ICD10 Code'].unique().tolist()\n",
        "\n",
        "# Generate embeddings for each unique ICD-10 description\n",
        "description_embeddings = {}\n",
        "for code in unique_icd10_codes:\n",
        "    description = code_to_description.get(code, \"\") # Get description, default to empty string if not found\n",
        "    if description: # Only process if description is not empty\n",
        "        encoded_description = tokenizer(description, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            output = model(**encoded_description, output_hidden_states=True)\n",
        "            last_hidden_states = output.hidden_hidden_states[-1]\n",
        "            # Take the mean of token embeddings as the description embedding\n",
        "            mean_description_embedding = last_hidden_states.mean(dim=1).squeeze().numpy() # Convert to numpy\n",
        "            description_embeddings[code] = mean_description_embedding\n",
        "    else:\n",
        "         description_embeddings[code] = np.zeros(model.config.hidden_size) # Use a zero vector for missing descriptions\n",
        "\n",
        "\n",
        "# Now, combine the note features with the corresponding description embeddings\n",
        "combined_features = []\n",
        "# Iterate through the filtered_df to ensure alignment with filtered_note_features\n",
        "for i, row in filtered_df.iterrows():\n",
        "    original_icd10_code = row['ICD10 Code']\n",
        "    note_feature_vec = filtered_note_features[filtered_df.index.get_loc(i)] # Get the correct note feature by index\n",
        "\n",
        "    desc_embedding = description_embeddings.get(original_icd10_code)\n",
        "\n",
        "    if desc_embedding is not None:\n",
        "        combined_feature_vec = np.concatenate((note_feature_vec, desc_embedding))\n",
        "        combined_features.append(combined_feature_vec)\n",
        "    else:\n",
        "        print(f\"Warning: No description embedding found for code {original_icd10_code} at index {i}\")\n",
        "        # Decide how to handle this - skip, use zero vector, etc.\n",
        "        pass # Or append np.concatenate((note_feature_vec, np.zeros(model.config.hidden_size)))\n",
        "\n",
        "\n",
        "combined_features = np.array(combined_features)\n",
        "\n",
        "print(\"Shape of combined features (note embeddings + description embeddings):\", combined_features.shape)\n",
        "\n",
        "# The labels should already be aligned as we iterated through filtered_df and used filtered_note_features\n",
        "# derived directly from it. However, if any samples were skipped in the combined_features loop,\n",
        "# we would need to filter the labels accordingly. Assuming no samples were skipped:\n",
        "filtered_encoded_labels_aligned = filtered_encoded_labels\n",
        "\n",
        "\n",
        "print(\"Shape of aligned labels:\", filtered_encoded_labels_aligned.shape)\n",
        "\n",
        "# Update filtered_note_features_combined to hold the new combined features\n",
        "filtered_note_features_combined = combined_features"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of recreated filtered_df: (66, 6)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'LogisticRegression' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-462098539.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mencoded_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnote\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mlast_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Take the mean of token embeddings as the note embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'LogisticRegression' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "339b8f72"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the variable `model` was overwritten with a `LogisticRegression` object in a previous subtask, but it was expected to be the ClinicalBERT model for generating embeddings. I need to reload the ClinicalBERT model to correct this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "y3J_6KWq7Q1U",
        "outputId": "8c1382a7-0ac7-4805-ae60-4d88280af6b9"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Reload the ClinicalBERT model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\", output_hidden_states=True)\n",
        "\n",
        "# Recreate filtered_df by applying the filtering steps from the previous subtask\n",
        "# Filter out rows where the ICD-10 code count is less than or equal to 1\n",
        "icd10_counts = df['ICD10 Code'].value_counts()\n",
        "codes_to_keep = icd10_counts[icd10_counts > 1].index\n",
        "filtered_df = df[df['ICD10 Code'].isin(codes_to_keep)].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "print(\"Shape of recreated filtered_df:\", filtered_df.shape)\n",
        "\n",
        "# Now, regenerate filtered_note_features and filtered_encoded_labels based on the recreated filtered_df\n",
        "# This step repeats part of the logic from the 'Train icd-10 code prediction model' subtask,\n",
        "# but is necessary to align features and labels with the recreated filtered_df.\n",
        "\n",
        "# Simulate specialist notes from the 'ICD10 Description ' column of the filtered_df\n",
        "filtered_notes = filtered_df['ICD10 Description '].tolist()\n",
        "\n",
        "# Process the simulated notes using the modified analyze_specialist_notes function\n",
        "# that returns a single feature vector (mean embedding) per note\n",
        "filtered_note_features = []\n",
        "for note in filtered_notes:\n",
        "    encoded_input = tokenizer(note, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        output = model(**encoded_input, output_hidden_states=True)\n",
        "        last_hidden_states = output.hidden_states[-1]\n",
        "        # Take the mean of token embeddings as the note embedding\n",
        "        mean_note_embedding = last_hidden_states.mean(dim=1).squeeze().numpy() # Convert to numpy\n",
        "        filtered_note_features.append(mean_note_embedding)\n",
        "\n",
        "filtered_note_features = np.array(filtered_note_features)\n",
        "\n",
        "print(\"Shape of regenerated filtered_note_features:\", filtered_note_features.shape)\n",
        "\n",
        "# Encode the 'ICD10 Code' from the filtered_df into numerical labels\n",
        "# Use LabelEncoder to ensure consistency with previous steps\n",
        "label_encoder = LabelEncoder()\n",
        "filtered_encoded_labels = label_encoder.fit_transform(filtered_df['ICD10 Code'])\n",
        "\n",
        "print(\"Shape of regenerated filtered_encoded_labels:\", filtered_encoded_labels.shape)\n",
        "\n",
        "# Now retry the logic from the previous failed command to generate description embeddings\n",
        "# and combine them with the note features.\n",
        "\n",
        "# Create a dictionary mapping ICD10 Code to its Description (handle potential duplicates by taking the first description)\n",
        "code_to_description = filtered_df.set_index('ICD10 Code')['ICD10 Description '].to_dict()\n",
        "\n",
        "# Get unique ICD-10 codes from the filtered data\n",
        "unique_icd10_codes = filtered_df['ICD10 Code'].unique().tolist()\n",
        "\n",
        "# Generate embeddings for each unique ICD-10 description\n",
        "description_embeddings = {}\n",
        "for code in unique_icd10_codes:\n",
        "    description = code_to_description.get(code, \"\") # Get description, default to empty string if not found\n",
        "    if description: # Only process if description is not empty\n",
        "        encoded_description = tokenizer(description, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            output = model(**encoded_description, output_hidden_states=True)\n",
        "            last_hidden_states = output.hidden_states[-1]\n",
        "            # Take the mean of token embeddings as the description embedding\n",
        "            mean_description_embedding = last_hidden_states.mean(dim=1).squeeze().numpy() # Convert to numpy\n",
        "            description_embeddings[code] = mean_description_embedding\n",
        "    else:\n",
        "         description_embeddings[code] = np.zeros(model.config.hidden_size) # Use a zero vector for missing descriptions\n",
        "\n",
        "\n",
        "# Now, combine the note features with the corresponding description embeddings\n",
        "combined_features = []\n",
        "# Iterate through the filtered_df to ensure alignment with filtered_note_features\n",
        "for i, row in filtered_df.iterrows():\n",
        "    original_icd10_code = row['ICD10 Code']\n",
        "    # Use the index from the original filtered_df to access the correct note feature\n",
        "    # Need to find the positional index in filtered_note_features array\n",
        "    # This mapping might be tricky if filtered_df was re-indexed.\n",
        "    # Let's assume the order in filtered_df is preserved in filtered_note_features\n",
        "    # A safer way is to iterate by index position if original index was dropped\n",
        "    # Let's try using the positional index directly from the loop\n",
        "    note_feature_vec = filtered_note_features[i] # This assumes filtered_df.iterrows() maintains order\n",
        "\n",
        "    desc_embedding = description_embeddings.get(original_icd10_code)\n",
        "\n",
        "    if desc_embedding is not None:\n",
        "        combined_feature_vec = np.concatenate((note_feature_vec, desc_embedding))\n",
        "        combined_features.append(combined_feature_vec)\n",
        "    else:\n",
        "        print(f\"Warning: No description embedding found for code {original_icd10_code} at index {i}\")\n",
        "        pass # Or append np.concatenate((note_feature_vec, np.zeros(model.config.hidden_size)))\n",
        "\n",
        "\n",
        "combined_features = np.array(combined_features)\n",
        "\n",
        "print(\"Shape of combined features (note embeddings + description embeddings):\", combined_features.shape)\n",
        "\n",
        "# The labels should already be aligned as we iterated through filtered_df and used filtered_note_features\n",
        "# derived directly from it. However, if any samples were skipped in the combined_features loop,\n",
        "# we would need to filter the labels accordingly. Assuming no samples were skipped:\n",
        "filtered_encoded_labels_aligned = filtered_encoded_labels\n",
        "\n",
        "\n",
        "print(\"Shape of aligned labels:\", filtered_encoded_labels_aligned.shape)\n",
        "\n",
        "# Update filtered_note_features_combined to hold the new combined features\n",
        "filtered_note_features_combined = combined_features"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of recreated filtered_df: (66, 6)\n",
            "Shape of regenerated filtered_note_features: (66, 768)\n",
            "Shape of regenerated filtered_encoded_labels: (66,)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 222 is out of bounds for axis 0 with size 66",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4207108566.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# A safer way is to iterate by index position if original index was dropped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# Let's try using the positional index directly from the loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mnote_feature_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiltered_note_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# This assumes filtered_df.iterrows() maintains order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mdesc_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdescription_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_icd10_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 222 is out of bounds for axis 0 with size 66"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88d682cc"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed with an `IndexError` because `filtered_df.iterrows()` returns the original index of the DataFrame, which does not correspond to the sequential positional index of the `filtered_note_features` NumPy array after filtering. I need to iterate through the `filtered_df` using its positional index or a list of indices to correctly access the corresponding note features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAXRz4y87XNK",
        "outputId": "8c881b9b-a15e-4e39-a63b-7b36cdea324a"
      },
      "source": [
        "# Recreate filtered_df by applying the filtering steps from the previous subtask\n",
        "# Filter out rows where the ICD-10 code count is less than or equal to 1\n",
        "icd10_counts = df['ICD10 Code'].value_counts()\n",
        "codes_to_keep = icd10_counts[icd10_counts > 1].index\n",
        "filtered_df = df[df['ICD10 Code'].isin(codes_to_keep)].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "print(\"Shape of recreated filtered_df:\", filtered_df.shape)\n",
        "\n",
        "# Now, regenerate filtered_note_features and filtered_encoded_labels based on the recreated filtered_df\n",
        "# This step repeats part of the logic from the 'Train icd-10 code prediction model' subtask,\n",
        "# but is necessary to align features and labels with the recreated filtered_df.\n",
        "\n",
        "# Simulate specialist notes from the 'ICD10 Description ' column of the filtered_df\n",
        "filtered_notes = filtered_df['ICD10 Description '].tolist()\n",
        "\n",
        "# Process the simulated notes using the modified analyze_specialist_notes function\n",
        "# that returns a single feature vector (mean embedding) per note\n",
        "filtered_note_features = []\n",
        "for note in filtered_notes:\n",
        "    encoded_input = tokenizer(note, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        output = model(**encoded_input, output_hidden_states=True)\n",
        "        last_hidden_states = output.hidden_states[-1]\n",
        "        # Take the mean of token embeddings as the note embedding\n",
        "        mean_note_embedding = last_hidden_states.mean(dim=1).squeeze().numpy() # Convert to numpy\n",
        "        filtered_note_features.append(mean_note_embedding)\n",
        "\n",
        "filtered_note_features = np.array(filtered_note_features)\n",
        "\n",
        "print(\"Shape of regenerated filtered_note_features:\", filtered_note_features.shape)\n",
        "\n",
        "# Encode the 'ICD10 Code' from the filtered_df into numerical labels\n",
        "# Use LabelEncoder to ensure consistency with previous steps\n",
        "label_encoder = LabelEncoder()\n",
        "filtered_encoded_labels = label_encoder.fit_transform(filtered_df['ICD10 Code'])\n",
        "\n",
        "print(\"Shape of regenerated filtered_encoded_labels:\", filtered_encoded_labels.shape)\n",
        "\n",
        "# Now retry the logic from the previous failed command to generate description embeddings\n",
        "# and combine them with the note features, correcting the indexing issue.\n",
        "\n",
        "# Create a dictionary mapping ICD10 Code to its Description (handle potential duplicates by taking the first description)\n",
        "code_to_description = filtered_df.set_index('ICD10 Code')['ICD10 Description '].to_dict()\n",
        "\n",
        "# Get unique ICD-10 codes from the filtered data\n",
        "unique_icd10_codes = filtered_df['ICD10 Code'].unique().tolist()\n",
        "\n",
        "# Generate embeddings for each unique ICD-10 description\n",
        "description_embeddings = {}\n",
        "for code in unique_icd10_codes:\n",
        "    description = code_to_description.get(code, \"\") # Get description, default to empty string if not found\n",
        "    if description: # Only process if description is not empty\n",
        "        encoded_description = tokenizer(description, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            output = model(**encoded_description, output_hidden_states=True)\n",
        "            # Access hidden_states correctly from the output object\n",
        "            last_hidden_states = output.hidden_states[-1]\n",
        "            # Take the mean of token embeddings as the description embedding\n",
        "            mean_description_embedding = last_hidden_states.mean(dim=1).squeeze().numpy() # Convert to numpy\n",
        "            description_embeddings[code] = mean_description_embedding\n",
        "    else:\n",
        "         description_embeddings[code] = np.zeros(model.config.hidden_size) # Use a zero vector for missing descriptions\n",
        "\n",
        "\n",
        "# Now, combine the note features with the corresponding description embeddings\n",
        "combined_features = []\n",
        "aligned_labels = [] # Also store aligned labels in case of skipping\n",
        "# Iterate through the filtered_df using its positional index\n",
        "for i in range(len(filtered_df)):\n",
        "    row = filtered_df.iloc[i] # Access row by positional index\n",
        "    original_icd10_code = row['ICD10 Code']\n",
        "    note_feature_vec = filtered_note_features[i] # Access note feature by positional index\n",
        "\n",
        "    desc_embedding = description_embeddings.get(original_icd10_code)\n",
        "\n",
        "    if desc_embedding is not None:\n",
        "        combined_feature_vec = np.concatenate((note_feature_vec, desc_embedding))\n",
        "        combined_features.append(combined_feature_vec)\n",
        "        aligned_labels.append(filtered_encoded_labels[i]) # Append corresponding label\n",
        "    else:\n",
        "        print(f\"Warning: No description embedding found for code {original_icd10_code} at index {i}\")\n",
        "        # Decide how to handle this - skip, use zero vector, etc.\n",
        "        pass # Skipping this sample\n",
        "\n",
        "\n",
        "combined_features = np.array(combined_features)\n",
        "aligned_labels = np.array(aligned_labels)\n",
        "\n",
        "print(\"Shape of combined features (note embeddings + description embeddings):\", combined_features.shape)\n",
        "print(\"Shape of aligned labels:\", aligned_labels.shape)\n",
        "\n",
        "# Update filtered_note_features_combined and filtered_encoded_labels_aligned\n",
        "# to hold the new combined features and aligned labels\n",
        "filtered_note_features_combined = combined_features\n",
        "filtered_encoded_labels_aligned = aligned_labels"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of recreated filtered_df: (66, 6)\n",
            "Shape of regenerated filtered_note_features: (66, 768)\n",
            "Shape of regenerated filtered_encoded_labels: (66,)\n",
            "Shape of combined features (note embeddings + description embeddings): (66, 1536)\n",
            "Shape of aligned labels: (66,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae42601b"
      },
      "source": [
        "## Integrate components\n",
        "\n",
        "### Subtask:\n",
        "Integrate the ClinicalBERT analysis, ICD-10 prediction, PMB pairing, and code differentiation logic into a single workflow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "689c31a2"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `authi_predict` function to integrate the ClinicalBERT analysis, ICD-10 prediction, PMB pairing, and code differentiation logic as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "2b34546d",
        "outputId": "237e3c27-28f0-4925-8403-523496e31a17"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "# Suppress ConvergenceWarning for Logistic Regression\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "# Ensure the ClinicalBERT model and tokenizer are loaded\n",
        "# It's necessary to reload them here if they were overwritten in previous steps\n",
        "# or if this function is intended to be standalone.\n",
        "try:\n",
        "    tokenizer\n",
        "    model_bert\n",
        "except NameError:\n",
        "    print(\"ClinicalBERT model and tokenizer not found. Loading...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "    # Load AutoModel, not AutoModelForSequenceClassification, for feature extraction\n",
        "    model_bert = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "    print(\"ClinicalBERT model and tokenizer loaded.\")\n",
        "\n",
        "# Assume 'label_encoder' and 'description_embeddings' are available from previous steps\n",
        "# Assume the trained prediction 'model' (Logistic Regression) is available\n",
        "# Assume the original 'df' DataFrame is available\n",
        "\n",
        "def analyze_specialist_notes(notes):\n",
        "    \"\"\"\n",
        "    Analyzes specialist notes using ClinicalBERT to extract a single feature vector\n",
        "    (mean of last hidden states).\n",
        "\n",
        "    Args:\n",
        "        notes (str): The specialist notes as a string.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A single feature vector (mean embedding) for the note.\n",
        "    \"\"\"\n",
        "    encoded_input = tokenizer(notes, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Use the loaded AutoModel (model_bert) for feature extraction\n",
        "        output = model_bert(**encoded_input, output_hidden_states=True)\n",
        "\n",
        "    last_hidden_states = output.hidden_states[-1]\n",
        "    # Take the mean of token embeddings as the note embedding\n",
        "    mean_note_embedding = last_hidden_states.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "    return mean_note_embedding\n",
        "\n",
        "def pair_icd10_with_pmb(predicted_icd10_code, dataframe):\n",
        "    \"\"\"\n",
        "    Pairs a predicted ICD-10 code with corresponding PMB information from a DataFrame.\n",
        "\n",
        "    Args:\n",
        "        predicted_icd10_code (str): The ICD-10 code predicted by the model.\n",
        "        dataframe (pd.DataFrame): The DataFrame containing ICD-10 and PMB information.\n",
        "\n",
        "    Returns:\n",
        "        list or str: A list of dictionaries, where each dictionary contains the\n",
        "                     'PMB Code' and 'PMB Description' for a matching row.\n",
        "                     Returns a string message if no match is found.\n",
        "    \"\"\"\n",
        "    # Ensure the input is treated as a string for comparison\n",
        "    predicted_icd10_code_str = str(predicted_icd10_code)\n",
        "\n",
        "    # Search the DataFrame for rows where 'ICD10 Code' matches the predicted code\n",
        "    # Ensure comparison is done with strings if 'ICD10 Code' might have mixed types\n",
        "    matching_rows = dataframe[dataframe['ICD10 Code'].astype(str) == predicted_icd10_code_str]\n",
        "\n",
        "    if matching_rows.empty:\n",
        "        return f\"No PMB information found for ICD-10 code: {predicted_icd10_code}\"\n",
        "    else:\n",
        "        # Extract 'PMB Code' and 'PMB Description' from matching rows\n",
        "        # Handle potential missing values by filling them or converting to string 'N/A'\n",
        "        pmb_info = []\n",
        "        for index, row in matching_rows.iterrows():\n",
        "            pmb_code = row['PMB Code'] if pd.notna(row['PMB Code']) else 'N/A'\n",
        "            pmb_description = row['PMB Description'] if pd.notna(row['PMB Description']) else 'N/A'\n",
        "            pmb_info.append({\n",
        "                'PMB Code': pmb_code,\n",
        "                'PMB Description': pmb_description\n",
        "            })\n",
        "        return pmb_info\n",
        "\n",
        "\n",
        "def authi_predict(specialist_note):\n",
        "    \"\"\"\n",
        "    Analyzes a specialist note, predicts the ICD-10 code, and pairs it with PMB information.\n",
        "\n",
        "    Args:\n",
        "        specialist_note (str): The specialist note to analyze.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the predicted ICD-10 code (str) and\n",
        "               the associated PMB information (list or str).\n",
        "               Returns (None, \"Error processing note\") if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Analyze the specialist note using ClinicalBERT\n",
        "        note_features = analyze_specialist_notes(specialist_note)\n",
        "\n",
        "        # 2. Prepare input features for the ICD-10 prediction model\n",
        "        # We need to get the description embedding for *each* possible ICD-10 code\n",
        "        # and then make a prediction for each code to find the most likely one.\n",
        "        # This approach is suitable for a trained classifier like Logistic Regression\n",
        "        # that expects a fixed input vector per prediction.\n",
        "\n",
        "        # Get the list of unique ICD-10 codes the model was trained on\n",
        "        trained_icd10_codes = label_encoder.classes_\n",
        "\n",
        "        # Prepare features for all possible codes\n",
        "        all_possible_combined_features = []\n",
        "        for code in trained_icd10_codes:\n",
        "            # Get the description embedding for the current code\n",
        "            # Use .get() with a default (e.g., zero vector) for safety\n",
        "            desc_embedding = description_embeddings.get(code, np.zeros(model_bert.config.hidden_size)) # Use model_bert.config.hidden_size\n",
        "\n",
        "            # Concatenate the note features with the description embedding\n",
        "            combined_feature_vec = np.concatenate((note_features, desc_embedding))\n",
        "            all_possible_combined_features.append(combined_feature_vec)\n",
        "\n",
        "        # Convert the list of features to a NumPy array\n",
        "        all_possible_combined_features = np.array(all_possible_combined_features)\n",
        "\n",
        "        # 3. Use the trained model to predict probabilities for each code\n",
        "        # Predict probabilities to find the most likely class\n",
        "        # The model expects input shape (n_samples, n_features)\n",
        "        # Here, n_samples is the number of unique ICD-10 codes the model was trained on.\n",
        "        # Reshape note_features if it's a single vector: note_features.reshape(1, -1)\n",
        "        # However, we prepared features for *all* possible codes, so use that array.\n",
        "        probabilities = model.predict_proba(all_possible_combined_features)\n",
        "\n",
        "        # Find the index of the class with the highest predicted probability\n",
        "        # probabilities has shape (n_samples, n_classes)\n",
        "        # Here, n_samples = number of unique trained ICD-10 codes\n",
        "        # n_classes = number of unique trained ICD-10 codes\n",
        "        # This interpretation is wrong. The model was trained to predict ONE class\n",
        "        # for a given combined feature vector (note + description).\n",
        "\n",
        "        # Let's rethink the prediction step. The trained model takes ONE combined\n",
        "        # feature vector (note + description for a *candidate* code) and predicts\n",
        "        # whether this pairing is correct (or predicts the class label directly).\n",
        "        # Given a new note, we want to find the BEST matching ICD-10 code.\n",
        "\n",
        "        # A common approach for this type of problem with combined features\n",
        "        # is to train a model that predicts the probability of a *match*\n",
        "        # between the note and a given ICD-10 code/description pair.\n",
        "        # Or, if the model is trained as a multi-class classifier directly\n",
        "        # on combined features (note + description), the input should represent\n",
        "        # the note and the target is the correct code.\n",
        "\n",
        "        # Based on the previous subtask's training (Logistic Regression on\n",
        "        # combined features predicting the encoded label), the model expects\n",
        "        # a single combined feature vector and outputs the predicted class label.\n",
        "        # This means the input to the model should be (note_embedding + description_embedding)\n",
        "        # for the *true* code during training. For prediction, this setup is not\n",
        "        # ideal for finding the best code among many, as you'd need to generate\n",
        "        # a combined feature vector for *each* possible code and somehow compare\n",
        "        # the model's output.\n",
        "\n",
        "        # Let's revert to a simpler interpretation that aligns with the training structure:\n",
        "        # The model was trained on (note_embedding_i + description_embedding_i) -> encoded_label_i.\n",
        "        # To predict for a new note, we need to generate features for that note.\n",
        "        # Then, to use the trained model, we would need to pair this note embedding\n",
        "        # with *each* possible description embedding and get a prediction for each pair.\n",
        "        # The prediction for a pair (note, description_j) would be the predicted class label.\n",
        "        # This still doesn't directly tell us which *code* is best for the note.\n",
        "\n",
        "        # A more appropriate model for this combined feature approach might be\n",
        "        # one that takes the note embedding and predicts probabilities over all\n",
        "        # possible ICD-10 codes. The description embeddings would be part of the\n",
        "        # training process, maybe implicitly through the combined features, or\n",
        "        # used in a different architecture (e.g., comparing note embedding to\n",
        "        # description embeddings).\n",
        "\n",
        "        # Given the current trained model (Logistic Regression on combined features -> encoded label),\n",
        "        # and assuming the combined features were (note_embedding || description_embedding_of_TRUE_CODE),\n",
        "        # the model learns to identify the correct code given the note and its true description.\n",
        "        # For prediction, we have a note but no true description.\n",
        "        # A practical way to use this model for prediction is to generate the note embedding,\n",
        "        # and then for each possible ICD-10 code/description pair, create the combined feature\n",
        "        # vector (note_embedding || description_embedding_of_CANDIDATE_CODE) and feed it\n",
        "        # to the model. The model's output (predicted class) for each candidate pair\n",
        "        # might indicate how well the note matches that description.\n",
        "\n",
        "        # Let's try the approach of generating combined features for the input note\n",
        "        # paired with the description of *each* possible ICD-10 code from the training set,\n",
        "        # and then use the model to predict the probability of the TRUE class for each pair.\n",
        "        # The candidate code that yields the highest probability for its own class\n",
        "        # when paired with the note might be the best prediction.\n",
        "\n",
        "        # Re-preparing features for prediction:\n",
        "        # For a single input note, we will create N combined feature vectors,\n",
        "        # where N is the number of unique ICD-10 codes the model was trained on.\n",
        "        # Each vector will be (note_embedding || description_embedding_for_code_i).\n",
        "\n",
        "        candidate_features = []\n",
        "        candidate_codes_list = []\n",
        "        for code in trained_icd10_codes:\n",
        "            desc_embedding = description_embeddings.get(code, np.zeros(model_bert.config.hidden_size))\n",
        "            combined_feature_vec = np.concatenate((note_features, desc_embedding))\n",
        "            candidate_features.append(combined_feature_vec)\n",
        "            candidate_codes_list.append(code)\n",
        "\n",
        "        candidate_features = np.array(candidate_features)\n",
        "\n",
        "        # Predict probabilities for each candidate combined feature vector\n",
        "        # The probabilities will be over the trained classes\n",
        "        candidate_probabilities = model.predict_proba(candidate_features)\n",
        "\n",
        "        # Find the probability of the *candidate code's own class* for each candidate feature vector\n",
        "        # The index of the candidate code in trained_icd10_codes corresponds to its index in the\n",
        "        # label_encoder classes and thus the columns of the probability array.\n",
        "        candidate_scores = []\n",
        "        for i, code in enumerate(candidate_codes_list):\n",
        "            try:\n",
        "                # Get the encoded label for the current candidate code\n",
        "                encoded_label = label_encoder.transform([code])[0]\n",
        "                # Get the probability of this encoded label from the model's output for this candidate feature\n",
        "                score = candidate_probabilities[i, encoded_label]\n",
        "                candidate_scores.append((score, code))\n",
        "            except ValueError:\n",
        "                 # This should not happen if candidate_codes_list comes from label_encoder.classes_\n",
        "                 print(f\"Warning: Candidate code {code} not found in label encoder classes.\")\n",
        "                 candidate_scores.append((-1, code)) # Assign a low score\n",
        "\n",
        "        # Sort candidates by score in descending order\n",
        "        candidate_scores.sort(key=lambda item: item[0], reverse=True)\n",
        "\n",
        "        # The predicted code is the one with the highest score\n",
        "        predicted_icd10_code = candidate_scores[0][1]\n",
        "        confidence_score = candidate_scores[0][0]\n",
        "        print(f\"Predicted ICD-10 Code: {predicted_icd10_code} with confidence score: {confidence_score:.4f}\")\n",
        "\n",
        "        # 5. Decode the predicted label back to the original ICD-10 code string\n",
        "        # This step is implicitly done by selecting from candidate_codes_list\n",
        "\n",
        "        # 6. Pair the predicted ICD-10 code with PMB information\n",
        "        pmb_info = pair_icd10_with_pmb(predicted_icd10_code, df) # Use the original df\n",
        "\n",
        "        # 7. Return the predicted ICD-10 code and PMB information\n",
        "        return predicted_icd10_code, pmb_info\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during prediction: {e}\")\n",
        "        return None, f\"Error processing note: {e}\"\n",
        "\n",
        "# Example usage (optional, for testing)\n",
        "# Assume 'df', 'label_encoder', 'description_embeddings', and 'model' (Logistic Regression) are available\n",
        "# test_note = \"Patient presents with symptoms indicative of drug-induced tremor.\"\n",
        "# predicted_code, pmb_details = authi_predict(test_note)\n",
        "# print(\"\\nPrediction Result:\")\n",
        "# print(\"Predicted ICD-10 Code:\", predicted_code)\n",
        "# print(\"PMB Information:\", pmb_details)\n",
        "\n",
        "# test_note_2 = \"Severe headache and neck stiffness, suspecting meningitis due to wild virus.\"\n",
        "# predicted_code_2, pmb_details_2 = authi_predict(test_note_2)\n",
        "# print(\"\\nPrediction Result 2:\")\n",
        "# print(\"Predicted ICD-10 Code:\", predicted_code_2)\n",
        "# print(\"PMB Information:\", pmb_details_2)\n",
        "\n",
        "# test_note_3 = \"Patient with history of eye trauma now experiencing increased intraocular pressure.\"\n",
        "# predicted_code_3, pmb_details_3 = authi_predict(test_note_3)\n",
        "# print(\"\\nPrediction Result 3:\")\n",
        "# print(\"Predicted ICD-10 Code:\", predicted_code_3)\n",
        "# print(\"PMB Information:\", pmb_details_3)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ClinicalBERT model and tokenizer not found. Loading...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AutoModel' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1041500550.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmodel_bert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_bert' is not defined",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1041500550.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"emilyalsentzer/Bio_ClinicalBERT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Load AutoModel, not AutoModelForSequenceClassification, for feature extraction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mmodel_bert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"emilyalsentzer/Bio_ClinicalBERT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ClinicalBERT model and tokenizer loaded.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoModel' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acb6b995"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `AutoModel` was not imported. Import `AutoModel` from `transformers` and regenerate the full code block for the `authi_predict` function definition.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fh3uRAg7o7J",
        "outputId": "9623ffdb-a116-47fc-d313-c7273bcbcb5b"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel # Import AutoModel\n",
        "import torch\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "# Import LogisticRegression and metrics if not already globally available and needed within the function\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# Suppress ConvergenceWarning for Logistic Regression\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "# Ensure the ClinicalBERT model and tokenizer are loaded\n",
        "# It's necessary to reload them here if they were overwritten in previous steps\n",
        "# or if this function is intended to be standalone.\n",
        "try:\n",
        "    tokenizer\n",
        "    model_bert\n",
        "except NameError:\n",
        "    print(\"ClinicalBERT model and tokenizer not found. Loading...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "    # Load AutoModel for feature extraction\n",
        "    model_bert = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "    print(\"ClinicalBERT model and tokenizer loaded.\")\n",
        "\n",
        "# Assume 'label_encoder' and 'description_embeddings' are available from previous steps\n",
        "# Assume the trained prediction 'model' (Logistic Regression) is available\n",
        "# Assume the original 'df' DataFrame is available\n",
        "\n",
        "def analyze_specialist_notes(notes):\n",
        "    \"\"\"\n",
        "    Analyzes specialist notes using ClinicalBERT to extract a single feature vector\n",
        "    (mean of last hidden states).\n",
        "\n",
        "    Args:\n",
        "        notes (str): The specialist notes as a string.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A single feature vector (mean embedding) for the note.\n",
        "    \"\"\"\n",
        "    encoded_input = tokenizer(notes, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Use the loaded AutoModel (model_bert) for feature extraction\n",
        "        output = model_bert(**encoded_input, output_hidden_states=True)\n",
        "\n",
        "    last_hidden_states = output.hidden_states[-1]\n",
        "    # Take the mean of token embeddings as the note embedding\n",
        "    mean_note_embedding = last_hidden_states.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "    return mean_note_embedding\n",
        "\n",
        "def pair_icd10_with_pmb(predicted_icd10_code, dataframe):\n",
        "    \"\"\"\n",
        "    Pairs a predicted ICD-10 code with corresponding PMB information from a DataFrame.\n",
        "\n",
        "    Args:\n",
        "        predicted_icd10_code (str): The ICD-10 code predicted by the model.\n",
        "        dataframe (pd.DataFrame): The DataFrame containing ICD-10 and PMB information.\n",
        "\n",
        "    Returns:\n",
        "        list or str: A list of dictionaries, where each dictionary contains the\n",
        "                     'PMB Code' and 'PMB Description' for a matching row.\n",
        "                     Returns a string message if no match is found.\n",
        "    \"\"\"\n",
        "    # Ensure the input is treated as a string for comparison\n",
        "    predicted_icd10_code_str = str(predicted_icd10_code)\n",
        "\n",
        "    # Search the DataFrame for rows where 'ICD10 Code' matches the predicted code\n",
        "    # Ensure comparison is done with strings if 'ICD10 Code' might have mixed types\n",
        "    matching_rows = dataframe[dataframe['ICD10 Code'].astype(str) == predicted_icd10_code_str]\n",
        "\n",
        "    if matching_rows.empty:\n",
        "        return f\"No PMB information found for ICD-10 code: {predicted_icd10_code}\"\n",
        "    else:\n",
        "        # Extract 'PMB Code' and 'PMB Description' from matching rows\n",
        "        # Handle potential missing values by filling them or converting to string 'N/A'\n",
        "        pmb_info = []\n",
        "        for index, row in matching_rows.iterrows():\n",
        "            pmb_code = row['PMB Code'] if pd.notna(row['PMB Code']) else 'N/A'\n",
        "            pmb_description = row['PMB Description'] if pd.notna(row['PMB Description']) else 'N/A'\n",
        "            pmb_info.append({\n",
        "                'PMB Code': pmb_code,\n",
        "                'PMB Description': pmb_description\n",
        "            })\n",
        "        return pmb_info\n",
        "\n",
        "\n",
        "def authi_predict(specialist_note):\n",
        "    \"\"\"\n",
        "    Analyzes a specialist note, predicts the ICD-10 code, and pairs it with PMB information.\n",
        "\n",
        "    Args:\n",
        "        specialist_note (str): The specialist note to analyze.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the predicted ICD-10 code (str) and\n",
        "               the associated PMB information (list or str).\n",
        "               Returns (None, \"Error processing note\") if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Analyze the specialist note using ClinicalBERT\n",
        "        note_features = analyze_specialist_notes(specialist_note)\n",
        "\n",
        "        # 2. Prepare input features for the ICD-10 prediction model\n",
        "        # We will create N combined feature vectors, where N is the number of\n",
        "        # unique ICD-10 codes the model was trained on. Each vector will be\n",
        "        # (note_embedding || description_embedding_for_code_i).\n",
        "\n",
        "        # Get the list of unique ICD-10 codes the model was trained on\n",
        "        trained_icd10_codes = label_encoder.classes_\n",
        "\n",
        "        candidate_features = []\n",
        "        candidate_codes_list = []\n",
        "        for code in trained_icd10_codes:\n",
        "            # Get the description embedding for the current code\n",
        "            # Use .get() with a default (e.g., zero vector) for safety\n",
        "            desc_embedding = description_embeddings.get(code, np.zeros(model_bert.config.hidden_size))\n",
        "\n",
        "            # Concatenate the note features with the description embedding\n",
        "            combined_feature_vec = np.concatenate((note_features, desc_embedding))\n",
        "            candidate_features.append(combined_feature_vec)\n",
        "            candidate_codes_list.append(code)\n",
        "\n",
        "        candidate_features = np.array(candidate_features)\n",
        "\n",
        "        # 3. Use the trained model to predict probabilities for each candidate combined feature vector\n",
        "        # The model predicts the likelihood of each class for a given input.\n",
        "        # For a candidate feature vector (note || desc_for_code_i), the model\n",
        "        # outputs probabilities over all trained classes.\n",
        "        candidate_probabilities = model.predict_proba(candidate_features)\n",
        "\n",
        "        # Find the probability of the *candidate code's own class* for each candidate feature vector\n",
        "        # The index of the candidate code in trained_icd10_codes corresponds to its index in the\n",
        "        # label_encoder classes and thus the columns of the probability array.\n",
        "        candidate_scores = []\n",
        "        for i, code in enumerate(candidate_codes_list):\n",
        "            try:\n",
        "                # Get the encoded label for the current candidate code\n",
        "                encoded_label = label_encoder.transform([code])[0]\n",
        "                # Get the probability of this encoded label (the code itself)\n",
        "                # from the model's output for this candidate feature vector (note + desc_for_code)\n",
        "                score = candidate_probabilities[i, encoded_label]\n",
        "                candidate_scores.append((score, code))\n",
        "            except ValueError:\n",
        "                 # This should not happen if candidate_codes_list comes from label_encoder.classes_\n",
        "                 print(f\"Warning: Candidate code {code} not found in label encoder classes.\")\n",
        "                 candidate_scores.append((-1, code)) # Assign a low score\n",
        "\n",
        "        # Sort candidates by score in descending order\n",
        "        candidate_scores.sort(key=lambda item: item[0], reverse=True)\n",
        "\n",
        "        # The predicted code is the one with the highest score\n",
        "        predicted_icd10_code = candidate_scores[0][1]\n",
        "        confidence_score = candidate_scores[0][0]\n",
        "        print(f\"Predicted ICD-10 Code: {predicted_icd10_code} with confidence score: {confidence_score:.4f}\")\n",
        "\n",
        "\n",
        "        # 5. Decode the predicted label back to the original ICD-10 code string\n",
        "        # This step is implicitly done by selecting from candidate_codes_list\n",
        "\n",
        "        # 6. Pair the predicted ICD-10 code with PMB information\n",
        "        pmb_info = pair_icd10_with_pmb(predicted_icd10_code, df) # Use the original df\n",
        "\n",
        "        # 7. Return the predicted ICD-10 code and PMB information\n",
        "        return predicted_icd10_code, pmb_info\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during prediction: {e}\")\n",
        "        return None, f\"Error processing note: {e}\"\n",
        "\n",
        "# Example usage (optional, for testing)\n",
        "# Assume 'df', 'label_encoder', 'description_embeddings', and 'model' (Logistic Regression) are available\n",
        "# test_note = \"Patient presents with symptoms indicative of drug-induced tremor.\"\n",
        "# predicted_code, pmb_details = authi_predict(test_note)\n",
        "# print(\"\\nPrediction Result:\")\n",
        "# print(\"Predicted ICD-10 Code:\", predicted_code)\n",
        "# print(\"PMB Information:\", pmb_details)\n",
        "\n",
        "# test_note_2 = \"Severe headache and neck stiffness, suspecting meningitis due to wild virus.\"\n",
        "# predicted_code_2, pmb_details_2 = authi_predict(test_note_2)\n",
        "# print(\"\\nPrediction Result 2:\")\n",
        "# print(\"Predicted ICD-10 Code:\", predicted_code_2)\n",
        "# print(\"PMB Information:\", pmb_details_2)\n",
        "\n",
        "# test_note_3 = \"Patient with history of eye trauma now experiencing increased intraocular pressure.\"\n",
        "# predicted_code_3, pmb_details_3 = authi_predict(test_note_3)\n",
        "# print(\"\\nPrediction Result 3:\")\n",
        "# print(\"Predicted ICD-10 Code:\", predicted_code_3)\n",
        "# print(\"PMB Information:\", pmb_details_3)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ClinicalBERT model and tokenizer not found. Loading...\n",
            "ClinicalBERT model and tokenizer loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a05363ec"
      },
      "source": [
        "## Evaluate and refine\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the integrated system and refine the models and logic as needed to improve accuracy and reliability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3821e3f"
      },
      "source": [
        "**Reasoning**:\n",
        "Prepare a test dataset from the existing filtered data and iterate through it to get predictions from the integrated system for evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 956
        },
        "id": "acee1676",
        "outputId": "b42fc6f8-2160-4780-cb3c-a9be16ff923d"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Check if filtered_df, filtered_encoded_labels_aligned, and filtered_note_features_combined are available\n",
        "if 'filtered_df' not in locals() or 'filtered_encoded_labels_aligned' not in locals() or 'filtered_note_features_combined' not in locals():\n",
        "    print(\"Required dataframes or arrays not found. Cannot proceed with evaluation.\")\n",
        "    # In a real scenario, you would regenerate these based on the original df\n",
        "else:\n",
        "    print(\"Required data found. Proceeding with test data preparation.\")\n",
        "\n",
        "    # Use a portion of the existing filtered data not used for training as the test set.\n",
        "    # Assuming the previous train_test_split used filtered_note_features_combined and filtered_encoded_labels_aligned\n",
        "    # We will split the combined features and aligned labels again.\n",
        "    # This split should be consistent with how the model was trained,\n",
        "    # but for evaluation, we'll use a new split or assume the previous test split is available.\n",
        "\n",
        "    # Let's assume the previous split resulted in X_train, X_test, y_train, y_test\n",
        "    # If those are not available, we'll perform a new split for evaluation purposes.\n",
        "    # A simple approach for evaluation is to use the same data used for training/testing in the previous step,\n",
        "    # but iterate through it using the authi_predict function.\n",
        "    # However, the instructions state \"a portion of the existing filtered data that was not used for training.\"\n",
        "    # Let's perform a new split focusing on creating a test set for this evaluation step.\n",
        "    # We will split the *original filtered_df* to get the specialist notes and ground truth codes.\n",
        "\n",
        "    # First, ensure the filtered_df is consistent with the features/labels used for training\n",
        "    # This means applying the same filtering logic (removing codes with count <= 1)\n",
        "    icd10_counts = df['ICD10 Code'].value_counts()\n",
        "    codes_to_keep = icd10_counts[icd10_counts > 1].index\n",
        "    filtered_df_eval = df[df['ICD10 Code'].isin(codes_to_keep)].copy()\n",
        "\n",
        "    # Split the filtered_df_eval into training and testing portions based on rows\n",
        "    # We need the original notes and ground truth ICD10 codes for evaluation\n",
        "    # Using stratify to maintain class distribution if possible, otherwise adjust\n",
        "    try:\n",
        "        train_df_eval, test_df_eval = train_test_split(filtered_df_eval, test_size=0.3, random_state=42, stratify=filtered_df_eval['ICD10 Code'])\n",
        "        print(f\"\\nTest set size: {len(test_df_eval)}\")\n",
        "        print(f\"Number of unique codes in test set: {test_df_eval['ICD10 Code'].nunique()}\")\n",
        "    except ValueError as e:\n",
        "        print(f\"Could not stratify split: {e}. Splitting without stratification.\")\n",
        "        train_df_eval, test_df_eval = train_test_split(filtered_df_eval, test_size=0.3, random_state=42)\n",
        "        print(f\"\\nTest set size: {len(test_df_eval)}\")\n",
        "        print(f\"Number of unique codes in test set: {test_df_eval['ICD10 Code'].nunique()}\")\n",
        "\n",
        "\n",
        "    # Prepare lists to store ground truth and predicted codes\n",
        "    ground_truth_codes = test_df_eval['ICD10 Code'].tolist()\n",
        "    predicted_codes = []\n",
        "\n",
        "    print(\"\\nEvaluating performance on the test set...\")\n",
        "    # Iterate through the test dataset\n",
        "    for index, row in test_df_eval.iterrows():\n",
        "        specialist_note = row['ICD10 Description '] # Using description as simulated note\n",
        "        true_icd10_code = row['ICD10 Code']\n",
        "\n",
        "        # Call the integrated prediction function\n",
        "        predicted_icd10_code, pmb_details = authi_predict(specialist_note)\n",
        "\n",
        "        # Store the predicted code\n",
        "        predicted_codes.append(predicted_icd10_code)\n",
        "\n",
        "        # Optional: Print details for each prediction for debugging\n",
        "        # print(f\"Note: {specialist_note}\")\n",
        "        # print(f\"Ground Truth: {true_icd10_code}, Predicted: {predicted_icd10_code}\")\n",
        "        # print(\"-\" * 20)\n",
        "\n",
        "    print(\"\\nEvaluation complete. Calculating metrics.\")\n",
        "\n",
        "    # Ensure both lists have the same length (should be equal to test_df_eval size)\n",
        "    if len(ground_truth_codes) != len(predicted_codes):\n",
        "        print(\"Error: Mismatch between ground truth and predicted codes length.\")\n",
        "    else:\n",
        "        # Calculate evaluation metrics\n",
        "        # Note: Metrics like precision, recall, and F1 are calculated on the\n",
        "        # encoded labels or by treating the string codes as classes.\n",
        "        # Using string codes directly with sklearn metrics requires handling\n",
        "        # potential classes present in predictions but not in ground truth (or vice versa).\n",
        "        # Let's use classification_report which handles this well.\n",
        "\n",
        "        # Handle potential None values in predicted_codes if authi_predict failed for some notes\n",
        "        valid_indices = [i for i, code in enumerate(predicted_codes) if code is not None]\n",
        "        ground_truth_codes_valid = [ground_truth_codes[i] for i in valid_indices]\n",
        "        predicted_codes_valid = [predicted_codes[i] for i in valid_indices]\n",
        "\n",
        "\n",
        "        print(\"\\nClassification Report:\")\n",
        "        # Generate classification report, which includes precision, recall, f1-score per class and averages\n",
        "        # Using zero_division=0 to avoid warnings/errors if a class has no predicted samples\n",
        "        print(classification_report(ground_truth_codes_valid, predicted_codes_valid, zero_division=0))\n",
        "\n",
        "        # Calculate overall accuracy\n",
        "        accuracy = accuracy_score(ground_truth_codes_valid, predicted_codes_valid)\n",
        "        print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # You can also calculate weighted averages explicitly if needed\n",
        "        # precision_weighted = precision_score(ground_truth_codes_valid, predicted_codes_valid, average='weighted', zero_division=0)\n",
        "        # recall_weighted = recall_score(ground_truth_codes_valid, predicted_codes_valid, average='weighted', zero_division=0)\n",
        "        # f1_weighted = f1_score(ground_truth_codes_valid, predicted_codes_valid, average='weighted', zero_division=0)\n",
        "\n",
        "        # print(f\"Weighted Precision: {precision_weighted:.4f}\")\n",
        "        # print(f\"Weighted Recall: {recall_weighted:.4f}\")\n",
        "        # print(f\"Weighted F1-Score: {f1_weighted:.4f}\")\n",
        "\n",
        "        # Store metrics for potential analysis (optional)\n",
        "        evaluation_metrics = {\n",
        "            'accuracy': accuracy,\n",
        "            # 'precision_weighted': precision_weighted,\n",
        "            # 'recall_weighted': recall_weighted,\n",
        "            # 'f1_weighted': f1_weighted\n",
        "        }\n",
        "        print(\"\\nEvaluation metrics calculated.\")\n",
        "\n",
        "        # Optional: Analyze misclassified cases\n",
        "        misclassified_notes = []\n",
        "        for i in range(len(ground_truth_codes_valid)):\n",
        "            if ground_truth_codes_valid[i] != predicted_codes_valid[i]:\n",
        "                 misclassified_notes.append({\n",
        "                     'note': test_df_eval.iloc[valid_indices[i]]['ICD10 Description '], # Get original note from df\n",
        "                     'ground_truth': ground_truth_codes_valid[i],\n",
        "                     'predicted': predicted_codes_valid[i]\n",
        "                 })\n",
        "\n",
        "        print(f\"\\nNumber of misclassified notes: {len(misclassified_notes)}\")\n",
        "        if misclassified_notes:\n",
        "            print(\"Examples of misclassified notes (showing first 5):\")\n",
        "            display(pd.DataFrame(misclassified_notes).head())\n",
        "\n",
        "\n",
        "    # Refinement considerations (Step 6 of instructions is covered by analysis in this step)\n",
        "    print(\"\\nAnalysis complete. Based on these results, potential refinements could include:\")\n",
        "    print(\"- Analyzing the classification report to identify classes with low precision/recall.\")\n",
        "    print(\"- Examining misclassified notes to understand why the predictions were incorrect.\")\n",
        "    print(\"- Exploring different classification models (e.g., SVM, Random Forest, or a simple neural network on combined features).\")\n",
        "    print(\"- Trying different strategies for combining note and description embeddings (e.g., concatenation, element-wise multiplication, attention).\")\n",
        "    print(\"- If feasible, gathering more labeled data, especially for underrepresented classes.\")\n",
        "    print(\"- Refining the ClinicalBERT feature extraction (e.g., using [CLS] token, pooling strategies).\")\n",
        "    print(\"- Investigating similarity-based approaches to code differentiation more deeply.\")\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Required data found. Proceeding with test data preparation.\n",
            "Could not stratify split: The test_size = 20 should be greater or equal to the number of classes = 33. Splitting without stratification.\n",
            "\n",
            "Test set size: 20\n",
            "Number of unique codes in test set: 17\n",
            "\n",
            "Evaluating performance on the test set...\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "\n",
            "Evaluation complete. Calculating metrics.\n",
            "\n",
            "Classification Report:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "max() arg is an empty sequence",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-24261803.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Generate classification report, which includes precision, recall, f1-score per class and averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# Using zero_division=0 to avoid warnings/errors if a class has no predicted samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mground_truth_codes_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_codes_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# Calculate overall accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2722\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m         \u001b[0mlongest_last_line_heading\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"weighted avg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2724\u001b[0;31m         \u001b[0mname_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2725\u001b[0m         \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlongest_last_line_heading\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2726\u001b[0m         \u001b[0mhead_fmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{:>{width}s} \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" {:>9}\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbd568d3"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to evaluate failed because the `model` variable was a `BertForSequenceClassification` object which does not have a `predict_proba` method, and also because all predictions were `None` leading to an empty sequence for the classification report. The `model` should be the trained `LogisticRegression` classifier, not the BERT model. I need to ensure the correct model is used for prediction and handle the case where `authi_predict` returns `None`. I will also fix the stratification issue by ensuring the test size is large enough.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVVi59sb70rb",
        "outputId": "5e83472c-7253-4e33-e2d8-14e8f2d7e8b1"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression # Ensure LogisticRegression is available if needed\n",
        "\n",
        "# Ensure the correct model (LogisticRegression) is available and loaded if necessary.\n",
        "# Assuming 'model' from the 'Train icd-10 code prediction model' subtask is the LogisticRegression model.\n",
        "# If not, it needs to be re-trained or loaded here.\n",
        "# For this fix, I'll assume 'model' is the trained LogisticRegression instance.\n",
        "\n",
        "# Ensure filtered_df is consistent with the features/labels used for training\n",
        "# This means applying the same filtering logic (removing codes with count <= 1)\n",
        "icd10_counts = df['ICD10 Code'].value_counts()\n",
        "codes_to_keep = icd10_counts[icd10_counts > 1].index\n",
        "filtered_df_eval = df[df['ICD10 Code'].isin(codes_to_keep)].copy()\n",
        "\n",
        "# Split the filtered_df_eval into training and testing portions based on rows\n",
        "# We need the original notes and ground truth ICD10 codes for evaluation\n",
        "# Adjust test_size to be large enough for stratification\n",
        "test_size_eval = 0.5 # Increased test size to ensure enough samples per class for stratification\n",
        "try:\n",
        "    train_df_eval, test_df_eval = train_test_split(filtered_df_eval, test_size=test_size_eval, random_state=42, stratify=filtered_df_eval['ICD10 Code'])\n",
        "    print(f\"\\nTest set size: {len(test_df_eval)}\")\n",
        "    print(f\"Number of unique codes in test set: {test_df_eval['ICD10 Code'].nunique()}\")\n",
        "except ValueError as e:\n",
        "    print(f\"Could not stratify split even with test_size={test_size_eval}: {e}. Splitting without stratification.\")\n",
        "    train_df_eval, test_df_eval = train_test_split(filtered_df_eval, test_size=test_size_eval, random_state=42)\n",
        "    print(f\"\\nTest set size: {len(test_df_eval)}\")\n",
        "    print(f\"Number of unique codes in test set: {test_df_eval['ICD10 Code'].nunique()}\")\n",
        "\n",
        "\n",
        "# Prepare lists to store ground truth and predicted codes\n",
        "ground_truth_codes = test_df_eval['ICD10 Code'].tolist()\n",
        "predicted_codes = []\n",
        "\n",
        "print(\"\\nEvaluating performance on the test set...\")\n",
        "# Iterate through the test dataset\n",
        "for index, row in test_df_eval.iterrows():\n",
        "    specialist_note = row['ICD10 Description '] # Using description as simulated note\n",
        "    true_icd10_code = row['ICD10 Code']\n",
        "\n",
        "    # Call the integrated prediction function\n",
        "    # Ensure the global 'model' variable used in authi_predict is the LogisticRegression model\n",
        "    predicted_icd10_code, pmb_details = authi_predict(specialist_note)\n",
        "\n",
        "    # Store the predicted code. Handle cases where authi_predict might return None.\n",
        "    predicted_codes.append(predicted_icd10_code)\n",
        "\n",
        "    # Optional: Print details for each prediction for debugging\n",
        "    # print(f\"Note: {specialist_note}\")\n",
        "    # print(f\"Ground Truth: {true_icd10_code}, Predicted: {predicted_icd10_code}\")\n",
        "    # print(\"-\" * 20)\n",
        "\n",
        "print(\"\\nEvaluation complete. Calculating metrics.\")\n",
        "\n",
        "# Filter out entries where prediction failed (predicted_code is None)\n",
        "valid_indices = [i for i, code in enumerate(predicted_codes) if code is not None]\n",
        "ground_truth_codes_valid = [ground_truth_codes[i] for i in valid_indices]\n",
        "predicted_codes_valid = [predicted_codes[i] for i in valid_indices]\n",
        "\n",
        "if not valid_indices:\n",
        "    print(\"No valid predictions were made. Cannot calculate metrics.\")\n",
        "else:\n",
        "    print(\"\\nClassification Report:\")\n",
        "    # Generate classification report, which includes precision, recall, f1-score per class and averages\n",
        "    # Using zero_division=0 to avoid warnings/errors if a class has no predicted samples\n",
        "    print(classification_report(ground_truth_codes_valid, predicted_codes_valid, zero_division=0))\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    accuracy = accuracy_score(ground_truth_codes_valid, predicted_codes_valid)\n",
        "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Optional: Analyze misclassified cases\n",
        "    misclassified_notes = []\n",
        "    for i in range(len(ground_truth_codes_valid)):\n",
        "        if ground_truth_codes_valid[i] != predicted_codes_valid[i]:\n",
        "             # Get the original row from test_df_eval using the original index\n",
        "             original_index = test_df_eval.iloc[valid_indices[i]].name\n",
        "             misclassified_notes.append({\n",
        "                 'note': test_df_eval.loc[original_index, 'ICD10 Description '],\n",
        "                 'ground_truth': ground_truth_codes_valid[i],\n",
        "                 'predicted': predicted_codes_valid[i]\n",
        "             })\n",
        "\n",
        "    print(f\"\\nNumber of misclassified notes: {len(misclassified_notes)}\")\n",
        "    if misclassified_notes:\n",
        "        print(\"Examples of misclassified notes (showing first 5):\")\n",
        "        display(pd.DataFrame(misclassified_notes).head())\n",
        "\n",
        "\n",
        "    # Refinement considerations (Step 6 of instructions is covered by analysis in this step)\n",
        "    print(\"\\nAnalysis complete. Based on these results, potential refinements could include:\")\n",
        "    print(\"- Analyzing the classification report to identify classes with low precision/recall.\")\n",
        "    print(\"- Examining misclassified notes to understand why the predictions were incorrect.\")\n",
        "    print(\"- Exploring different classification models (e.g., SVM, Random Forest, or a simple neural network on combined features).\")\n",
        "    print(\"- Trying different strategies for combining note and description embeddings (e.g., concatenation, element-wise multiplication, attention).\")\n",
        "    print(\"- If feasible, gathering more labeled data, especially for underrepresented classes.\")\n",
        "    print(\"- Refining the ClinicalBERT feature extraction (e.g., using [CLS] token, pooling strategies).\")\n",
        "    print(\"- Investigating similarity-based approaches to code differentiation more deeply.\")\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set size: 33\n",
            "Number of unique codes in test set: 33\n",
            "\n",
            "Evaluating performance on the test set...\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "\n",
            "Evaluation complete. Calculating metrics.\n",
            "No valid predictions were made. Cannot calculate metrics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bef0ba4"
      },
      "source": [
        "**Reasoning**:\n",
        "The error \"AttributeError: 'BertForSequenceClassification' object has no attribute 'predict_proba'\" indicates that the `model` variable inside the `authi_predict` function is referencing the BERT model loaded for feature extraction (`model_bert`) instead of the trained Logistic Regression model. I need to fix the `authi_predict` function to use the correct `model` object (the Logistic Regression classifier) for the `predict_proba` call.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8dfe2eb",
        "outputId": "57499804-88b5-421b-fb56-e2fbfe9f5bf7"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression # Ensure LogisticRegression is available\n",
        "\n",
        "# Ensure the correct model (LogisticRegression) is available and loaded if necessary.\n",
        "# Assuming 'model' from the 'Train icd-10 code prediction model' subtask is the LogisticRegression model.\n",
        "# If not, it needs to be re-trained or loaded here.\n",
        "# For this fix, I'll assume 'model' is the trained LogisticRegression instance.\n",
        "\n",
        "# Ensure the ClinicalBERT model and tokenizer are loaded and correctly named for feature extraction\n",
        "try:\n",
        "    tokenizer\n",
        "    model_bert # This should be the AutoModel for feature extraction\n",
        "except NameError:\n",
        "    print(\"ClinicalBERT model and tokenizer not found. Loading...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "    # Load AutoModel for feature extraction\n",
        "    model_bert = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "    print(\"ClinicalBERT model and tokenizer loaded.\")\n",
        "\n",
        "# Assume 'label_encoder' and 'description_embeddings' are available from previous steps\n",
        "# Assume the trained prediction 'model' (Logistic Regression) is available\n",
        "\n",
        "def analyze_specialist_notes(notes):\n",
        "    \"\"\"\n",
        "    Analyzes specialist notes using ClinicalBERT to extract a single feature vector\n",
        "    (mean of last hidden states).\n",
        "\n",
        "    Args:\n",
        "        notes (str): The specialist notes as a string.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A single feature vector (mean embedding) for the note.\n",
        "    \"\"\"\n",
        "    encoded_input = tokenizer(notes, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Use the loaded AutoModel (model_bert) for feature extraction\n",
        "        output = model_bert(**encoded_input, output_hidden_states=True)\n",
        "\n",
        "    last_hidden_states = output.hidden_states[-1]\n",
        "    # Take the mean of token embeddings as the note embedding\n",
        "    mean_note_embedding = last_hidden_states.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "    return mean_note_embedding\n",
        "\n",
        "def pair_icd10_with_pmb(predicted_icd10_code, dataframe):\n",
        "    \"\"\"\n",
        "    Pairs a predicted ICD-10 code with corresponding PMB information from a DataFrame.\n",
        "\n",
        "    Args:\n",
        "        predicted_icd10_code (str): The ICD-10 code predicted by the model.\n",
        "        dataframe (pd.DataFrame): The DataFrame containing ICD-10 and PMB information.\n",
        "\n",
        "    Returns:\n",
        "        list or str: A list of dictionaries, where each dictionary contains the\n",
        "                     'PMB Code' and 'PMB Description' for a matching row.\n",
        "                     Returns a string message if no match is found.\n",
        "    \"\"\"\n",
        "    # Ensure the input is treated as a string for comparison\n",
        "    predicted_icd10_code_str = str(predicted_icd10_code)\n",
        "\n",
        "    # Search the DataFrame for rows where 'ICD10 Code' matches the predicted code\n",
        "    # Ensure comparison is done with strings if 'ICD10 Code' might have mixed types\n",
        "    matching_rows = dataframe[dataframe['ICD10 Code'].astype(str) == predicted_icd10_code_str]\n",
        "\n",
        "    if matching_rows.empty:\n",
        "        return f\"No PMB information found for ICD-10 code: {predicted_icd10_code}\"\n",
        "    else:\n",
        "        # Extract 'PMB Code' and 'PMB Description' from matching rows\n",
        "        # Handle potential missing values by filling them or converting to string 'N/A'\n",
        "        pmb_info = []\n",
        "        for index, row in matching_rows.iterrows():\n",
        "            pmb_code = row['PMB Code'] if pd.notna(row['PMB Code']) else 'N/A'\n",
        "            pmb_description = row['PMB Description'] if pd.notna(row['PMB Description']) else 'N/A'\n",
        "            pmb_info.append({\n",
        "                'PMB Code': pmb_code,\n",
        "                'PMB Description': pmb_description\n",
        "            })\n",
        "        return pmb_info\n",
        "\n",
        "\n",
        "def authi_predict(specialist_note, prediction_model, label_encoder, description_embeddings_dict, dataframe_for_pmb):\n",
        "    \"\"\"\n",
        "    Analyzes a specialist note, predicts the ICD-10 code, and pairs it with PMB information.\n",
        "\n",
        "    Args:\n",
        "        specialist_note (str): The specialist note to analyze.\n",
        "        prediction_model: The trained classification model (e.g., LogisticRegression).\n",
        "        label_encoder (LabelEncoder): The LabelEncoder used to encode ICD-10 codes.\n",
        "        description_embeddings_dict (dict): A dictionary mapping ICD-10 codes to their embeddings.\n",
        "        dataframe_for_pmb (pd.DataFrame): The DataFrame containing ICD-10 and PMB information.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the predicted ICD-10 code (str) and\n",
        "               the associated PMB information (list or str).\n",
        "               Returns (None, f\"Error processing note: {e}\") if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Analyze the specialist note using ClinicalBERT\n",
        "        note_features = analyze_specialist_notes(specialist_note)\n",
        "\n",
        "        # 2. Prepare input features for the ICD-10 prediction model\n",
        "        # We will create N combined feature vectors, where N is the number of\n",
        "        # unique ICD-10 codes the model was trained on. Each vector will be\n",
        "        # (note_embedding || description_embedding_for_code_i).\n",
        "\n",
        "        # Get the list of unique ICD-10 codes the model was trained on\n",
        "        trained_icd10_codes = label_encoder.classes_\n",
        "\n",
        "        candidate_features = []\n",
        "        candidate_codes_list = []\n",
        "        for code in trained_icd10_codes:\n",
        "            # Get the description embedding for the current code\n",
        "            # Use .get() with a default (e.g., zero vector) for safety\n",
        "            desc_embedding = description_embeddings_dict.get(code, np.zeros(model_bert.config.hidden_size))\n",
        "\n",
        "            # Concatenate the note features with the description embedding\n",
        "            combined_feature_vec = np.concatenate((note_features, desc_embedding))\n",
        "            candidate_features.append(combined_feature_vec)\n",
        "            candidate_codes_list.append(code)\n",
        "\n",
        "        candidate_features = np.array(candidate_features)\n",
        "\n",
        "        # 3. Use the trained prediction_model to predict probabilities for each candidate combined feature vector\n",
        "        # FIX: Use the passed 'prediction_model' parameter here, not the global 'model'\n",
        "        candidate_probabilities = prediction_model.predict_proba(candidate_features)\n",
        "\n",
        "        # Find the probability of the *candidate code's own class* for each candidate feature vector\n",
        "        # The index of the candidate code in trained_icd10_codes corresponds to its index in the\n",
        "        # label_encoder classes and thus the columns of the probability array.\n",
        "        candidate_scores = []\n",
        "        for i, code in enumerate(candidate_codes_list):\n",
        "            try:\n",
        "                # Get the encoded label for the current candidate code\n",
        "                encoded_label = label_encoder.transform([code])[0]\n",
        "                # Get the probability of this encoded label (the code itself)\n",
        "                # from the model's output for this candidate feature vector (note + desc_for_code)\n",
        "                score = candidate_probabilities[i, encoded_label]\n",
        "                candidate_scores.append((score, code))\n",
        "            except ValueError:\n",
        "                 # This should not happen if candidate_codes_list comes from label_encoder.classes_\n",
        "                 print(f\"Warning: Candidate code {code} not found in label encoder classes.\")\n",
        "                 candidate_scores.append((-1, code)) # Assign a low score\n",
        "\n",
        "        # Sort candidates by score in descending order\n",
        "        candidate_scores.sort(key=lambda item: item[0], reverse=True)\n",
        "\n",
        "        # The predicted code is the one with the highest score\n",
        "        predicted_icd10_code = candidate_scores[0][1]\n",
        "        confidence_score = candidate_scores[0][0]\n",
        "        # print(f\"Predicted ICD-10 Code: {predicted_icd10_code} with confidence score: {confidence_score:.4f}\")\n",
        "\n",
        "\n",
        "        # 5. Decode the predicted label back to the original ICD-10 code string\n",
        "        # This step is implicitly done by selecting from candidate_codes_list\n",
        "\n",
        "        # 6. Pair the predicted ICD-10 code with PMB information\n",
        "        pmb_info = pair_icd10_with_pmb(predicted_icd10_code, dataframe_for_pmb) # Use the original df\n",
        "\n",
        "        # 7. Return the predicted ICD-10 code and PMB information\n",
        "        return predicted_icd10_code, pmb_info\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during prediction: {e}\")\n",
        "        return None, f\"Error processing note: {e}\"\n",
        "\n",
        "# Now, re-run the evaluation loop with the corrected authi_predict function\n",
        "# and pass the necessary objects (model, label_encoder, description_embeddings, df)\n",
        "\n",
        "# Ensure filtered_df is consistent with the features/labels used for training\n",
        "# This means applying the same filtering logic (removing codes with count <= 1)\n",
        "icd10_counts = df['ICD10 Code'].value_counts()\n",
        "codes_to_keep = icd10_counts[icd10_counts > 1].index\n",
        "filtered_df_eval = df[df['ICD10 Code'].isin(codes_to_keep)].copy()\n",
        "\n",
        "# Split the filtered_df_eval into training and testing portions based on rows\n",
        "# We need the original notes and ground truth ICD10 codes for evaluation\n",
        "# Adjust test_size to be large enough for stratification\n",
        "test_size_eval = 0.5 # Increased test size to ensure enough samples per class for stratification\n",
        "try:\n",
        "    train_df_eval, test_df_eval = train_test_split(filtered_df_eval, test_size=test_size_eval, random_state=42, stratify=filtered_df_eval['ICD10 Code'])\n",
        "    print(f\"\\nTest set size: {len(test_df_eval)}\")\n",
        "    print(f\"Number of unique codes in test set: {test_df_eval['ICD10 Code'].nunique()}\")\n",
        "except ValueError as e:\n",
        "    print(f\"Could not stratify split even with test_size={test_size_eval}: {e}. Splitting without stratification.\")\n",
        "    train_df_eval, test_df_eval = train_test_split(filtered_df_eval, test_size=test_size_eval, random_state=42)\n",
        "    print(f\"\\nTest set size: {len(test_df_eval)}\")\n",
        "    print(f\"Number of unique codes in test set: {test_df_eval['ICD10 Code'].nunique()}\")\n",
        "\n",
        "\n",
        "# Prepare lists to store ground truth and predicted codes\n",
        "ground_truth_codes = test_df_eval['ICD10 Code'].tolist()\n",
        "predicted_codes = []\n",
        "\n",
        "print(\"\\nEvaluating performance on the test set...\")\n",
        "# Iterate through the test dataset\n",
        "for index, row in test_df_eval.iterrows():\n",
        "    specialist_note = row['ICD10 Description '] # Using description as simulated note\n",
        "    true_icd10_code = row['ICD10 Code']\n",
        "\n",
        "    # Call the integrated prediction function, passing the required objects\n",
        "    predicted_icd10_code, pmb_details = authi_predict(\n",
        "        specialist_note=specialist_note,\n",
        "        prediction_model=model, # Pass the trained Logistic Regression model\n",
        "        label_encoder=label_encoder,\n",
        "        description_embeddings_dict=description_embeddings,\n",
        "        dataframe_for_pmb=df # Pass the original df for PMB lookup\n",
        "    )\n",
        "\n",
        "    # Store the predicted code. Handle cases where authi_predict might return None.\n",
        "    predicted_codes.append(predicted_icd10_code)\n",
        "\n",
        "    # Optional: Print details for each prediction for debugging\n",
        "    # print(f\"Note: {specialist_note}\")\n",
        "    # print(f\"Ground Truth: {true_icd10_code}, Predicted: {predicted_icd10_code}\")\n",
        "    # print(\"-\" * 20)\n",
        "\n",
        "print(\"\\nEvaluation complete. Calculating metrics.\")\n",
        "\n",
        "# Filter out entries where prediction failed (predicted_code is None)\n",
        "valid_indices = [i for i, code in enumerate(predicted_codes) if code is not None]\n",
        "ground_truth_codes_valid = [ground_truth_codes[i] for i in valid_indices]\n",
        "predicted_codes_valid = [predicted_codes[i] for i in valid_indices]\n",
        "\n",
        "if not valid_indices:\n",
        "    print(\"No valid predictions were made. Cannot calculate metrics.\")\n",
        "else:\n",
        "    print(\"\\nClassification Report:\")\n",
        "    # Generate classification report, which includes precision, recall, f1-score per class and averages\n",
        "    # Using zero_division=0 to avoid warnings/errors if a class has no predicted samples\n",
        "    print(classification_report(ground_truth_codes_valid, predicted_codes_valid, zero_division=0))\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    accuracy = accuracy_score(ground_truth_codes_valid, predicted_codes_valid)\n",
        "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Optional: Analyze misclassified cases\n",
        "    misclassified_notes = []\n",
        "    for i in range(len(ground_truth_codes_valid)):\n",
        "        if ground_truth_codes_valid[i] != predicted_codes_valid[i]:\n",
        "             # Get the original row from test_df_eval using the original index\n",
        "             original_index = test_df_eval.iloc[valid_indices[i]].name\n",
        "             misclassified_notes.append({\n",
        "                 'note': test_df_eval.loc[original_index, 'ICD10 Description '],\n",
        "                 'ground_truth': ground_truth_codes_valid[i],\n",
        "                 'predicted': predicted_codes_valid[i]\n",
        "             })\n",
        "\n",
        "    print(f\"\\nNumber of misclassified notes: {len(misclassified_notes)}\")\n",
        "    if misclassified_notes:\n",
        "        print(\"Examples of misclassified notes (showing first 5):\")\n",
        "        display(pd.DataFrame(misclassified_notes).head())\n",
        "\n",
        "\n",
        "    # Refinement considerations (Step 6 of instructions is covered by analysis in this step)\n",
        "    print(\"\\nAnalysis complete. Based on these results, potential refinements could include:\")\n",
        "    print(\"- Analyzing the classification report to identify classes with low precision/recall.\")\n",
        "    print(\"- Examining misclassified notes to understand why the predictions were incorrect.\")\n",
        "    print(\"- Exploring different classification models (e.g., SVM, Random Forest, or a simple neural network on combined features).\")\n",
        "    print(\"- Trying different strategies for combining note and description embeddings (e.g., concatenation, element-wise multiplication, attention).\")\n",
        "    print(\"- If feasible, gathering more labeled data, especially for underrepresented classes.\")\n",
        "    print(\"- Refining the ClinicalBERT feature extraction (e.g., using [CLS] token, pooling strategies).\")\n",
        "    print(\"- Investigating similarity-based approaches to code differentiation more deeply.\")\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set size: 33\n",
            "Number of unique codes in test set: 33\n",
            "\n",
            "Evaluating performance on the test set...\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "An error occurred during prediction: 'BertForSequenceClassification' object has no attribute 'predict_proba'\n",
            "\n",
            "Evaluation complete. Calculating metrics.\n",
            "No valid predictions were made. Cannot calculate metrics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6011b100"
      },
      "source": [
        "**Reasoning**:\n",
        "The error persists because the variable `model` inside the `authi_predict` function (despite being passed as `prediction_model`) is still somehow being overwritten or referenced incorrectly as the `BertForSequenceClassification` object. I need to ensure that the `model` variable that holds the trained Logistic Regression classifier is correctly available and passed to the `authi_predict` function. I will explicitly retrain the Logistic Regression model before calling the evaluation loop to make sure `model` holds the correct trained object.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4c300f1f",
        "outputId": "4d9dae8b-d1e7-4417-c8f5-4835d15b8699"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression # Ensure LogisticRegression is available\n",
        "from sklearn.preprocessing import LabelEncoder # Ensure LabelEncoder is available\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "# Suppress ConvergenceWarning for Logistic Regression\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "# Ensure the ClinicalBERT model and tokenizer are loaded and correctly named for feature extraction\n",
        "try:\n",
        "    tokenizer\n",
        "    model_bert # This should be the AutoModel for feature extraction\n",
        "except NameError:\n",
        "    print(\"ClinicalBERT model and tokenizer not found. Loading...\")\n",
        "    from transformers import AutoTokenizer, AutoModel\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "    # Load AutoModel for feature extraction\n",
        "    model_bert = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
        "    print(\"ClinicalBERT model and tokenizer loaded.\")\n",
        "\n",
        "# Assume 'label_encoder' and 'description_embeddings' are available from previous steps\n",
        "# If not, they need to be regenerated\n",
        "if 'label_encoder' not in locals() or 'description_embeddings' not in locals() or 'filtered_note_features_combined' not in locals() or 'filtered_encoded_labels_aligned' not in locals():\n",
        "     print(\"Label encoder, description embeddings, or combined features/labels not found. Regenerating...\")\n",
        "     # Recreate filtered_df by applying the filtering steps\n",
        "     icd10_counts = df['ICD10 Code'].value_counts()\n",
        "     codes_to_keep = icd10_counts[icd10_counts > 1].index\n",
        "     filtered_df = df[df['ICD10 Code'].isin(codes_to_keep)].copy()\n",
        "\n",
        "     # Regenerate filtered_note_features\n",
        "     filtered_notes = filtered_df['ICD10 Description '].tolist()\n",
        "     filtered_note_features = []\n",
        "     for note in filtered_notes:\n",
        "         encoded_input = tokenizer(note, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "         with torch.no_grad():\n",
        "             output = model_bert(**encoded_input, output_hidden_states=True)\n",
        "             last_hidden_states = output.hidden_states[-1]\n",
        "             mean_note_embedding = last_hidden_states.mean(dim=1).squeeze().numpy()\n",
        "             filtered_note_features.append(mean_note_embedding)\n",
        "     filtered_note_features = np.array(filtered_note_features)\n",
        "\n",
        "     # Regenerate label_encoder and filtered_encoded_labels\n",
        "     label_encoder = LabelEncoder()\n",
        "     filtered_encoded_labels = label_encoder.fit_transform(filtered_df['ICD10 Code'])\n",
        "\n",
        "     # Regenerate description_embeddings\n",
        "     code_to_description = filtered_df.set_index('ICD10 Code')['ICD10 Description '].to_dict()\n",
        "     unique_icd10_codes = filtered_df['ICD10 Code'].unique().tolist()\n",
        "     description_embeddings = {}\n",
        "     for code in unique_icd10_codes:\n",
        "         description = code_to_description.get(code, \"\")\n",
        "         if description:\n",
        "             encoded_description = tokenizer(description, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "             with torch.no_grad():\n",
        "                 output = model_bert(**encoded_description, output_hidden_states=True)\n",
        "                 last_hidden_states = output.hidden_states[-1]\n",
        "                 mean_description_embedding = last_hidden_states.mean(dim=1).squeeze().numpy()\n",
        "                 description_embeddings[code] = mean_description_embedding\n",
        "         else:\n",
        "              description_embeddings[code] = np.zeros(model_bert.config.hidden_size)\n",
        "\n",
        "     # Regenerate combined features and aligned labels\n",
        "     combined_features = []\n",
        "     aligned_labels = []\n",
        "     for i in range(len(filtered_df)):\n",
        "         row = filtered_df.iloc[i]\n",
        "         original_icd10_code = row['ICD10 Code']\n",
        "         note_feature_vec = filtered_note_features[i]\n",
        "         desc_embedding = description_embeddings.get(original_icd10_code)\n",
        "         if desc_embedding is not None:\n",
        "             combined_feature_vec = np.concatenate((note_feature_vec, desc_embedding))\n",
        "             combined_features.append(combined_feature_vec)\n",
        "             aligned_labels.append(filtered_encoded_labels[i])\n",
        "     filtered_note_features_combined = np.array(combined_features)\n",
        "     filtered_encoded_labels_aligned = np.array(aligned_labels)\n",
        "     print(\"Regeneration complete.\")\n",
        "\n",
        "\n",
        "def analyze_specialist_notes(notes):\n",
        "    \"\"\"\n",
        "    Analyzes specialist notes using ClinicalBERT to extract a single feature vector\n",
        "    (mean of last hidden states).\n",
        "\n",
        "    Args:\n",
        "        notes (str): The specialist notes as a string.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: A single feature vector (mean embedding) for the note.\n",
        "    \"\"\"\n",
        "    encoded_input = tokenizer(notes, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Use the loaded AutoModel (model_bert) for feature extraction\n",
        "        output = model_bert(**encoded_input, output_hidden_states=True)\n",
        "\n",
        "    last_hidden_states = output.hidden_states[-1]\n",
        "    # Take the mean of token embeddings as the note embedding\n",
        "    mean_note_embedding = last_hidden_states.mean(dim=1).squeeze().numpy()\n",
        "\n",
        "    return mean_note_embedding\n",
        "\n",
        "def pair_icd10_with_pmb(predicted_icd10_code, dataframe):\n",
        "    \"\"\"\n",
        "    Pairs a predicted ICD-10 code with corresponding PMB information from a DataFrame.\n",
        "\n",
        "    Args:\n",
        "        predicted_icd10_code (str): The ICD-10 code predicted by the model.\n",
        "        dataframe (pd.DataFrame): The DataFrame containing ICD-10 and PMB information.\n",
        "\n",
        "    Returns:\n",
        "        list or str: A list of dictionaries, where each dictionary contains the\n",
        "                     'PMB Code' and 'PMB Description' for a matching row.\n",
        "                     Returns a string message if no match is found.\n",
        "    \"\"\"\n",
        "    # Ensure the input is treated as a string for comparison\n",
        "    predicted_icd10_code_str = str(predicted_icd10_code)\n",
        "\n",
        "    # Search the DataFrame for rows where 'ICD10 Code' matches the predicted code\n",
        "    # Ensure comparison is done with strings if 'ICD10 Code' might have mixed types\n",
        "    matching_rows = dataframe[dataframe['ICD10 Code'].astype(str) == predicted_icd10_code_str]\n",
        "\n",
        "    if matching_rows.empty:\n",
        "        return f\"No PMB information found for ICD-10 code: {predicted_icd10_code}\"\n",
        "    else:\n",
        "        # Extract 'PMB Code' and 'PMB Description' from matching rows\n",
        "        # Handle potential missing values by filling them or converting to string 'N/A'\n",
        "        pmb_info = []\n",
        "        for index, row in matching_rows.iterrows():\n",
        "            pmb_code = row['PMB Code'] if pd.notna(row['PMB Code']) else 'N/A'\n",
        "            pmb_description = row['PMB Description'] if pd.notna(row['PMB Description']) else 'N/A'\n",
        "            pmb_info.append({\n",
        "                'PMB Code': pmb_code,\n",
        "                'PMB Description': pmb_description\n",
        "            })\n",
        "        return pmb_info\n",
        "\n",
        "\n",
        "def authi_predict(specialist_note, prediction_model, label_encoder, description_embeddings_dict, dataframe_for_pmb):\n",
        "    \"\"\"\n",
        "    Analyzes a specialist note, predicts the ICD-10 code, and pairs it with PMB information.\n",
        "\n",
        "    Args:\n",
        "        specialist_note (str): The specialist note to analyze.\n",
        "        prediction_model: The trained classification model (e.g., LogisticRegression).\n",
        "        label_encoder (LabelEncoder): The LabelEncoder used to encode ICD-10 codes.\n",
        "        description_embeddings_dict (dict): A dictionary mapping ICD-10 codes to their embeddings.\n",
        "        dataframe_for_pmb (pd.DataFrame): The DataFrame containing ICD-10 and PMB information.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the predicted ICD-10 code (str) and\n",
        "               the associated PMB information (list or str).\n",
        "               Returns (None, f\"Error processing note: {e}\") if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. Analyze the specialist note using ClinicalBERT\n",
        "        note_features = analyze_specialist_notes(specialist_note)\n",
        "\n",
        "        # 2. Prepare input features for the ICD-10 prediction model\n",
        "        # We will create N combined feature vectors, where N is the number of\n",
        "        # unique ICD-10 codes the model was trained on. Each vector will be\n",
        "        # (note_embedding || description_embedding_for_code_i).\n",
        "\n",
        "        # Get the list of unique ICD-10 codes the model was trained on\n",
        "        trained_icd10_codes = label_encoder.classes_\n",
        "\n",
        "        candidate_features = []\n",
        "        candidate_codes_list = []\n",
        "        for code in trained_icd10_codes:\n",
        "            # Get the description embedding for the current code\n",
        "            # Use .get() with a default (e.g., zero vector) for safety\n",
        "            desc_embedding = description_embeddings_dict.get(code, np.zeros(model_bert.config.hidden_size))\n",
        "\n",
        "            # Concatenate the note features with the description embedding\n",
        "            combined_feature_vec = np.concatenate((note_features, desc_embedding))\n",
        "            candidate_features.append(combined_feature_vec)\n",
        "            candidate_codes_list.append(code)\n",
        "\n",
        "        candidate_features = np.array(candidate_features)\n",
        "\n",
        "        # 3. Use the trained prediction_model to predict probabilities for each candidate combined feature vector\n",
        "        candidate_probabilities = prediction_model.predict_proba(candidate_features)\n",
        "\n",
        "        # Find the probability of the *candidate code's own class* for each candidate feature vector\n",
        "        # The index of the candidate code in trained_icd10_codes corresponds to its index in the\n",
        "        # label_encoder classes and thus the columns of the probability array.\n",
        "        candidate_scores = []\n",
        "        for i, code in enumerate(candidate_codes_list):\n",
        "            try:\n",
        "                # Get the encoded label for the current candidate code\n",
        "                encoded_label = label_encoder.transform([code])[0]\n",
        "                # Get the probability of this encoded label (the code itself)\n",
        "                # from the model's output for this candidate feature vector (note + desc_for_code)\n",
        "                score = candidate_probabilities[i, encoded_label]\n",
        "                candidate_scores.append((score, code))\n",
        "            except ValueError:\n",
        "                 # This should not happen if candidate_codes_list comes from label_encoder.classes_\n",
        "                 print(f\"Warning: Candidate code {code} not found in label encoder classes.\")\n",
        "                 candidate_scores.append((-1, code)) # Assign a low score\n",
        "\n",
        "        # Sort candidates by score in descending order\n",
        "        candidate_scores.sort(key=lambda item: item[0], reverse=True)\n",
        "\n",
        "        # The predicted code is the one with the highest score\n",
        "        predicted_icd10_code = candidate_scores[0][1]\n",
        "        confidence_score = candidate_scores[0][0]\n",
        "        # print(f\"Predicted ICD-10 Code: {predicted_icd10_code} with confidence score: {confidence_score:.4f}\")\n",
        "\n",
        "\n",
        "        # 5. Decode the predicted label back to the original ICD-10 code string\n",
        "        # This step is implicitly done by selecting from candidate_codes_list\n",
        "\n",
        "        # 6. Pair the predicted ICD-10 code with PMB information\n",
        "        pmb_info = pair_icd10_with_pmb(predicted_icd10_code, dataframe_for_pmb) # Use the original df\n",
        "\n",
        "        # 7. Return the predicted ICD-10 code and PMB information\n",
        "        return predicted_icd10_code, pmb_info\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during prediction: {e}\")\n",
        "        return None, f\"Error processing note: {e}\"\n",
        "\n",
        "\n",
        "# --- Evaluation Code ---\n",
        "\n",
        "# Ensure filtered_df is consistent with the features/labels used for training\n",
        "# This means applying the same filtering logic (removing codes with count <= 1)\n",
        "icd10_counts = df['ICD10 Code'].value_counts()\n",
        "codes_to_keep = icd10_counts[icd10_counts > 1].index\n",
        "filtered_df_eval = df[df['ICD10 Code'].isin(codes_to_keep)].copy()\n",
        "\n",
        "# Split the filtered_df_eval into training and testing portions based on rows\n",
        "# We need the original notes and ground truth ICD10 codes for evaluation\n",
        "# Adjust test_size to be large enough for stratification\n",
        "test_size_eval = 0.5 # Increased test size to ensure enough samples per class for stratification\n",
        "try:\n",
        "    train_df_eval, test_df_eval = train_test_split(filtered_df_eval, test_size=test_size_eval, random_state=42, stratify=filtered_df_eval['ICD10 Code'])\n",
        "    print(f\"\\nTest set size: {len(test_df_eval)}\")\n",
        "    print(f\"Number of unique codes in test set: {test_df_eval['ICD10 Code'].nunique()}\")\n",
        "except ValueError as e:\n",
        "    print(f\"Could not stratify split even with test_size={test_size_eval}: {e}. Splitting without stratification.\")\n",
        "    train_df_eval, test_df_eval = train_test_split(filtered_df_eval, test_size=test_size_eval, random_state=42)\n",
        "    print(f\"\\nTest set size: {len(test_df_eval)}\")\n",
        "    print(f\"Number of unique codes in test set: {test_df_eval['ICD10 Code'].nunique()}\")\n",
        "\n",
        "# Retrain the Logistic Regression model using the training split for evaluation\n",
        "# This ensures the 'model' variable holds the correct trained classifier for the current evaluation run.\n",
        "# We need the training features and labels for this.\n",
        "# Assuming the previous split of combined features also used test_size=0.5 and random_state=42\n",
        "# If not, we need to regenerate the train/test split for combined features here as well.\n",
        "# For simplicity and consistency with the evaluation split of the dataframe:\n",
        "X_train_eval, X_test_eval, y_train_eval, y_test_eval = train_test_split(\n",
        "    filtered_note_features_combined,\n",
        "    filtered_encoded_labels_aligned,\n",
        "    test_size=test_size_eval,\n",
        "    random_state=42,\n",
        "    # Use stratify if the dataframe split was stratified, otherwise omit\n",
        "    # stratify=filtered_encoded_labels_aligned # This requires the same indices as the dataframe split, which might be tricky\n",
        "    # Let's assume a simple non-stratified split for the features/labels for now, matching the non-stratified dataframe split if stratification fails\n",
        ")\n",
        "if 'train_df_eval' in locals() and len(np.unique(train_df_eval['ICD10 Code'])) == len(np.unique(filtered_encoded_labels_aligned)):\n",
        "     try:\n",
        "        X_train_eval, X_test_eval, y_train_eval, y_test_eval = train_test_split(\n",
        "            filtered_note_features_combined,\n",
        "            filtered_encoded_labels_aligned,\n",
        "            test_size=test_size_eval,\n",
        "            random_state=42,\n",
        "            stratify=filtered_encoded_labels_aligned # Attempt stratification if dataframe split was stratified\n",
        "        )\n",
        "        print(\"Successfully stratified split for features/labels for retraining.\")\n",
        "     except ValueError:\n",
        "        print(\"Could not stratify split for features/labels for retraining. Splitting without stratification.\")\n",
        "        X_train_eval, X_test_eval, y_train_eval, y_test_eval = train_test_split(\n",
        "            filtered_note_features_combined,\n",
        "            filtered_encoded_labels_aligned,\n",
        "            test_size=test_size_eval,\n",
        "            random_state=42,\n",
        "        )\n",
        "else:\n",
        "    print(\"Could not match stratification due to dataframe split issues. Splitting features/labels without stratification.\")\n",
        "    X_train_eval, X_test_eval, y_train_eval, y_test_eval = train_test_split(\n",
        "        filtered_note_features_combined,\n",
        "        filtered_encoded_labels_aligned,\n",
        "        test_size=test_size_eval,\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "\n",
        "# Retrain the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000, solver='liblinear')\n",
        "model.fit(X_train_eval, y_train_eval)\n",
        "print(\"\\nLogistic Regression model retrained for evaluation.\")\n",
        "\n",
        "\n",
        "# Prepare lists to store ground truth and predicted codes\n",
        "ground_truth_codes = test_df_eval['ICD10 Code'].tolist()\n",
        "predicted_codes = []\n",
        "\n",
        "print(\"\\nEvaluating performance on the test set using the retrained model...\")\n",
        "# Iterate through the test dataset\n",
        "for index, row in test_df_eval.iterrows():\n",
        "    specialist_note = row['ICD10 Description '] # Using description as simulated note\n",
        "    true_icd10_code = row['ICD10 Code']\n",
        "\n",
        "    # Call the integrated prediction function, passing the required objects\n",
        "    predicted_icd10_code, pmb_details = authi_predict(\n",
        "        specialist_note=specialist_note,\n",
        "        prediction_model=model, # Pass the retrained Logistic Regression model\n",
        "        label_encoder=label_encoder,\n",
        "        description_embeddings_dict=description_embeddings,\n",
        "        dataframe_for_pmb=df # Pass the original df for PMB lookup\n",
        "    )\n",
        "\n",
        "    # Store the predicted code. Handle cases where authi_predict might return None.\n",
        "    predicted_codes.append(predicted_icd10_code)\n",
        "\n",
        "    # Optional: Print details for each prediction for debugging\n",
        "    # print(f\"Note: {specialist_note}\")\n",
        "    # print(f\"Ground Truth: {true_icd10_code}, Predicted: {predicted_icd10_code}\")\n",
        "    # print(\"-\" * 20)\n",
        "\n",
        "print(\"\\nEvaluation complete. Calculating metrics.\")\n",
        "\n",
        "# Filter out entries where prediction failed (predicted_code is None)\n",
        "valid_indices = [i for i, code in enumerate(predicted_codes) if code is not None]\n",
        "ground_truth_codes_valid = [ground_truth_codes[i] for i in valid_indices]\n",
        "predicted_codes_valid = [predicted_codes[i] for i in valid_indices]\n",
        "\n",
        "if not valid_indices:\n",
        "    print(\"No valid predictions were made. Cannot calculate metrics.\")\n",
        "else:\n",
        "    print(\"\\nClassification Report:\")\n",
        "    # Generate classification report, which includes precision, recall, f1-score per class and averages\n",
        "    # Using zero_division=0 to avoid warnings/errors if a class has no predicted samples\n",
        "    print(classification_report(ground_truth_codes_valid, predicted_codes_valid, zero_division=0))\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    accuracy = accuracy_score(ground_truth_codes_valid, predicted_codes_valid)\n",
        "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Optional: Analyze misclassified cases\n",
        "    misclassified_notes = []\n",
        "    for i in range(len(ground_truth_codes_valid)):\n",
        "        if ground_truth_codes_valid[i] != predicted_codes_valid[i]:\n",
        "             # Get the original row from test_df_eval using the original index\n",
        "             original_index = test_df_eval.iloc[valid_indices[i]].name\n",
        "             misclassified_notes.append({\n",
        "                 'note': test_df_eval.loc[original_index, 'ICD10 Description '],\n",
        "                 'ground_truth': ground_truth_codes_valid[i],\n",
        "                 'predicted': predicted_codes_valid[i]\n",
        "             })\n",
        "\n",
        "    print(f\"\\nNumber of misclassified notes: {len(misclassified_notes)}\")\n",
        "    if misclassified_notes:\n",
        "        print(\"Examples of misclassified notes (showing first 5):\")\n",
        "        display(pd.DataFrame(misclassified_notes).head())\n",
        "\n",
        "\n",
        "    # Refinement considerations (Step 6 of instructions is covered by analysis in this step)\n",
        "    print(\"\\nAnalysis complete. Based on these results, potential refinements could include:\")\n",
        "    print(\"- Analyzing the classification report to identify classes with low precision/recall.\")\n",
        "    print(\"- Examining misclassified notes to understand why the predictions were incorrect.\")\n",
        "    print(\"- Exploring different classification models (e.g., SVM, Random Forest, or a simple neural network on combined features).\")\n",
        "    print(\"- Trying different strategies for combining note and description embeddings (e.g., concatenation, element-wise multiplication, attention).\")\n",
        "    print(\"- If feasible, gathering more labeled data, especially for underrepresented classes.\")\n",
        "    print(\"- Refining the ClinicalBERT feature extraction (e.g., using [CLS] token, pooling strategies).\")\n",
        "    print(\"- Investigating similarity-based approaches to code differentiation more deeply.\")\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set size: 33\n",
            "Number of unique codes in test set: 33\n",
            "Successfully stratified split for features/labels for retraining.\n",
            "\n",
            "Logistic Regression model retrained for evaluation.\n",
            "\n",
            "Evaluating performance on the test set using the retrained model...\n",
            "\n",
            "Evaluation complete. Calculating metrics.\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       G25.1       1.00      1.00      1.00         1\n",
            "       G25.3       1.00      1.00      1.00         1\n",
            "       G25.4       1.00      1.00      1.00         1\n",
            "       G25.5       1.00      1.00      1.00         1\n",
            "       G25.6       1.00      1.00      1.00         1\n",
            "       G45.8       1.00      1.00      1.00         1\n",
            "       G45.9       1.00      1.00      1.00         1\n",
            "       G93.5       1.00      1.00      1.00         1\n",
            "       G93.6       1.00      1.00      1.00         1\n",
            "       H40.3       1.00      1.00      1.00         1\n",
            "       H40.5       1.00      1.00      1.00         1\n",
            "       H40.8       1.00      1.00      1.00         1\n",
            "       H44.0       1.00      1.00      1.00         1\n",
            "       H44.1       1.00      1.00      1.00         1\n",
            "       H44.6       1.00      1.00      1.00         1\n",
            "       H44.7       1.00      1.00      1.00         1\n",
            "I62.9+G46.3*       0.00      0.00      0.00         1\n",
            "I62.9+G46.4*       0.50      1.00      0.67         1\n",
            "      S06.40       0.00      0.00      0.00         1\n",
            "      S06.41       0.00      0.00      0.00         1\n",
            "      S06.50       0.00      0.00      0.00         1\n",
            "      S06.51       0.00      0.00      0.00         1\n",
            "      S06.60       1.00      1.00      1.00         1\n",
            "      S06.61       0.00      0.00      0.00         1\n",
            "       S34.0       1.00      1.00      1.00         1\n",
            "       S34.1       1.00      1.00      1.00         1\n",
            "       S34.6       1.00      1.00      1.00         1\n",
            "       S34.8       1.00      1.00      1.00         1\n",
            "      T02.00       0.50      1.00      0.67         1\n",
            "       T06.0       1.00      1.00      1.00         1\n",
            "       T26.3       1.00      1.00      1.00         1\n",
            "       T26.4       1.00      1.00      1.00         1\n",
            "       T26.7       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           0.82        33\n",
            "   macro avg       0.79      0.82      0.80        33\n",
            "weighted avg       0.79      0.82      0.80        33\n",
            "\n",
            "Overall Accuracy: 0.8182\n",
            "\n",
            "Number of misclassified notes: 6\n",
            "Examples of misclassified notes (showing first 5):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                                note ground_truth predicted\n",
              "0  Epidural haemorrhage with open intracranial in...       S06.41    S06.40\n",
              "1  Traumatic subarachnoid haemorrhage, with open ...       S06.61    T02.00\n",
              "2  Epidural haemorrhage, without open intracrania...       S06.40    S06.41\n",
              "3  Traumatic subdural haemorrhage, without open i...       S06.50    S06.51\n",
              "4  Traumatic subdural haemorrhage with open intra...       S06.51    S06.50"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-29bc0328-a3a7-4397-b8c1-16cedf160532\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>note</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Epidural haemorrhage with open intracranial in...</td>\n",
              "      <td>S06.41</td>\n",
              "      <td>S06.40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Traumatic subarachnoid haemorrhage, with open ...</td>\n",
              "      <td>S06.61</td>\n",
              "      <td>T02.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Epidural haemorrhage, without open intracrania...</td>\n",
              "      <td>S06.40</td>\n",
              "      <td>S06.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Traumatic subdural haemorrhage, without open i...</td>\n",
              "      <td>S06.50</td>\n",
              "      <td>S06.51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Traumatic subdural haemorrhage with open intra...</td>\n",
              "      <td>S06.51</td>\n",
              "      <td>S06.50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29bc0328-a3a7-4397-b8c1-16cedf160532')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-29bc0328-a3a7-4397-b8c1-16cedf160532 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-29bc0328-a3a7-4397-b8c1-16cedf160532');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-a05206b3-6b04-4c67-aeb0-10d45e7ef191\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a05206b3-6b04-4c67-aeb0-10d45e7ef191')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-a05206b3-6b04-4c67-aeb0-10d45e7ef191 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(\\\"- Investigating similarity-based approaches to code differentiation more deeply\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"note\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Traumatic subarachnoid haemorrhage, with open intracranial wound\",\n          \"Traumatic subdural haemorrhage with open intracranial injury\",\n          \"Epidural haemorrhage, without open intracranial wound\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truth\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"S06.61\",\n          \"S06.51\",\n          \"S06.40\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"predicted\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"T02.00\",\n          \"S06.50\",\n          \"S06.41\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analysis complete. Based on these results, potential refinements could include:\n",
            "- Analyzing the classification report to identify classes with low precision/recall.\n",
            "- Examining misclassified notes to understand why the predictions were incorrect.\n",
            "- Exploring different classification models (e.g., SVM, Random Forest, or a simple neural network on combined features).\n",
            "- Trying different strategies for combining note and description embeddings (e.g., concatenation, element-wise multiplication, attention).\n",
            "- If feasible, gathering more labeled data, especially for underrepresented classes.\n",
            "- Refining the ClinicalBERT feature extraction (e.g., using [CLS] token, pooling strategies).\n",
            "- Investigating similarity-based approaches to code differentiation more deeply.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "665f0363"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial data loading encountered a `UnicodeDecodeError` but was successfully resolved by using 'latin1' encoding.\n",
        "*   The dataset contained missing values, particularly in the 'PMB Code' and 'PMB Description' columns.\n",
        "*   A `KeyError` during data cleaning revealed a trailing space in the 'ICD10 Description ' column name.\n",
        "*   Training the initial Logistic Regression model for ICD-10 prediction on ClinicalBERT features resulted in an accuracy of approximately 84.85% on a filtered dataset.\n",
        "*   Stratified splitting of the data for training and testing was challenging due to a small number of samples per class and was successfully addressed by filtering out single-instance classes and adjusting the test set size.\n",
        "*   A function was successfully developed to pair predicted ICD-10 codes with PMB information from the dataset, handling cases with no match or multiple matches.\n",
        "*   Incorporating ICD-10 description embeddings alongside note embeddings was implemented as a strategy to enhance code differentiation.\n",
        "*   The integrated prediction function (`authi_predict`) was successfully created, combining note analysis, code prediction using combined features, and PMB pairing.\n",
        "*   Evaluation of the integrated system showed an overall accuracy of 0.8182 on the test set.\n",
        "*   The evaluation process highlighted challenges in differentiating between similar ICD-10 codes, as evidenced by the classification report and analysis of misclassified notes.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Analyze the classification report and misclassified notes in detail to identify patterns and specific areas for improvement, focusing on classes with low precision and recall.\n",
        "*   Explore alternative classification models or feature combination strategies (e.g., different pooling methods for ClinicalBERT, attention mechanisms, or neural network architectures) that might better leverage the combined note and description embeddings to improve differentiation between similar codes.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b3b4f92d33404aebab4613f48d583c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f9a9ee610da4a2c9145e655e9dc9f7e",
              "IPY_MODEL_9a39efbdf17b4f80af0f6b5747aabf04",
              "IPY_MODEL_29f360a92c3f4994b531415ffcb899c9"
            ],
            "layout": "IPY_MODEL_5acf4420bf434ceaa312da833f16bfad"
          }
        },
        "0f9a9ee610da4a2c9145e655e9dc9f7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61950a467c5f404e86065e3fb65b337a",
            "placeholder": "",
            "style": "IPY_MODEL_36c64f260b1b4e6d8a1f64346107db5b",
            "value": "config.json:100%"
          }
        },
        "9a39efbdf17b4f80af0f6b5747aabf04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bd26d61fc864809a4519d5f26124bfd",
            "max": 385,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd26a642341d48119ca0ebf372d61bc3",
            "value": 385
          }
        },
        "29f360a92c3f4994b531415ffcb899c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_393d59139b2f4a2aafaca8935105913c",
            "placeholder": "",
            "style": "IPY_MODEL_e2061a2ccdb84ababf9b0b4b3d5d1c73",
            "value": "385/385[00:00&lt;00:00,23.9kB/s]"
          }
        },
        "5acf4420bf434ceaa312da833f16bfad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61950a467c5f404e86065e3fb65b337a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36c64f260b1b4e6d8a1f64346107db5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bd26d61fc864809a4519d5f26124bfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd26a642341d48119ca0ebf372d61bc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "393d59139b2f4a2aafaca8935105913c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2061a2ccdb84ababf9b0b4b3d5d1c73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "296c16cc3e83449f84fab669ec4dbc39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_511815e9a1824227b5e6ad7d53c55db4",
              "IPY_MODEL_1ffe93b911a649508cef813fd3c23b0a",
              "IPY_MODEL_f5d5790700b1411088167dc26386a783"
            ],
            "layout": "IPY_MODEL_184de08451714f43a74a7614f78c891e"
          }
        },
        "511815e9a1824227b5e6ad7d53c55db4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8ac571d2f424acf988bb0913489d3be",
            "placeholder": "",
            "style": "IPY_MODEL_886af8372aea4bc284a30f322e103daf",
            "value": "vocab.txt:"
          }
        },
        "1ffe93b911a649508cef813fd3c23b0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_863bd357904a40169b8278630586cde6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8078fbc294cc4507874888c27ccb5492",
            "value": 1
          }
        },
        "f5d5790700b1411088167dc26386a783": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acf417c9c240435a983a26897edd1c1a",
            "placeholder": "",
            "style": "IPY_MODEL_885eb29b111949fa881b39f8b85ca7d4",
            "value": "213k/?[00:00&lt;00:00,4.18MB/s]"
          }
        },
        "184de08451714f43a74a7614f78c891e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8ac571d2f424acf988bb0913489d3be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "886af8372aea4bc284a30f322e103daf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "863bd357904a40169b8278630586cde6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8078fbc294cc4507874888c27ccb5492": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "acf417c9c240435a983a26897edd1c1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "885eb29b111949fa881b39f8b85ca7d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecc845b8a59d4ce48a0efa94ef383c7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e17c0e8a3714be2905a3292d8994867",
              "IPY_MODEL_9d6b9f7c006a41b1a0bf02dad1de026e",
              "IPY_MODEL_f77adc1a14a348bc8386216257050b95"
            ],
            "layout": "IPY_MODEL_bb3fc67a084543de84a11290da38d2c0"
          }
        },
        "8e17c0e8a3714be2905a3292d8994867": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d43fd048c9f4eecadf73f825845b2d5",
            "placeholder": "",
            "style": "IPY_MODEL_48026425ab8e473a9e0febd53e78f810",
            "value": "pytorch_model.bin:100%"
          }
        },
        "9d6b9f7c006a41b1a0bf02dad1de026e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_809b84fb5d804893b4f4b5bd3ec432f4",
            "max": 435778770,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54338b94319f4d01938c9fd7e6855893",
            "value": 435778770
          }
        },
        "f77adc1a14a348bc8386216257050b95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27a546ff1446477dac77857e81d64d32",
            "placeholder": "",
            "style": "IPY_MODEL_a64db286f1d446aeb1982d3ff5dc8ed3",
            "value": "436M/436M[00:05&lt;00:00,83.2MB/s]"
          }
        },
        "bb3fc67a084543de84a11290da38d2c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d43fd048c9f4eecadf73f825845b2d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48026425ab8e473a9e0febd53e78f810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "809b84fb5d804893b4f4b5bd3ec432f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54338b94319f4d01938c9fd7e6855893": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27a546ff1446477dac77857e81d64d32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a64db286f1d446aeb1982d3ff5dc8ed3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}